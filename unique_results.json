[
  {
    "score": 0.17586879432201385,
    "title": "Google DeepMind on X: \"Today, we‚Äôre announcing Veo 2: our state-of-the-art video generation model which produces realistic, high-quality clips from text or image prompts. üé•\n\nWe‚Äôre also releasing an improved version of our text-to-image model, Imagen 3 - available to use in ImageFX through https://t.co/h6ejHaMUM4\" / X",
    "id": "https://x.com/GoogleDeepMind/status/1868703624714395907",
    "url": "https://x.com/GoogleDeepMind/status/1868703624714395907",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "2024-12-16 None Conversation read image description read image description 5:03 PM ¬∑ Dec 16, 2024",
    "summary": "Google DeepMind announced two AI model updates today: Veo 2, a state-of-the-art video generation model creating realistic clips from text or image prompts, and an improved version of their text-to-image model, Imagen 3, accessible through ImageFX.\n"
  },
  {
    "score": 0.1749265193939209,
    "title": "",
    "id": "https://twitter.com/GoogleDeepMind/status/1868703624714395907",
    "url": "https://twitter.com/GoogleDeepMind/status/1868703624714395907",
    "publishedDate": "2024-12-16T17:03:57.000Z",
    "author": "GoogleDeepMind",
    "text": "Today, we‚Äôre announcing Veo 2: our state-of-the-art video generation model which produces realistic, high-quality clips from text or image prompts. üé• We‚Äôre also releasing an improved version of our text-to-image model, Imagen 3 - available to use in ImageFX through https://t.co/h6ejHaMUM4| created_at: Mon Dec 16 17:03:57 +0000 2024 | favorite_count: 3466 | quote_count: 279 | reply_count: 135 | retweet_count: 688 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "Google DeepMind announced two AI model updates today: Veo 2, a state-of-the-art video generation model creating realistic clips from text or image prompts; and an improved version of Imagen 3, their text-to-image model, accessible via ImageFX.\n"
  },
  {
    "score": 0.16554605960845947,
    "title": "",
    "id": "https://twitter.com/OpenAI/status/1868715324885156177",
    "url": "https://twitter.com/OpenAI/status/1868715324885156177",
    "publishedDate": "2024-12-16T17:50:27.000Z",
    "author": "OpenAI",
    "text": "Day 8: ChatGPT Search Day https://t.co/mpGI9mMqtn| created_at: Mon Dec 16 17:50:27 +0000 2024 | favorite_count: 1061 | quote_count: 78 | reply_count: 218 | retweet_count: 109 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "OpenAI tweeted on December 16th, 2024 about \"ChatGPT Search Day,\"  providing a link (but not details about specific AI updates released that day).\n"
  },
  {
    "score": 0.16543348133563995,
    "title": "Google‚Äôs Whisk AI generator will ‚Äòremix‚Äô the pictures you plug in",
    "id": "https://www.theverge.com/2024/12/16/24322614/google-whisk-ai-generator-remix-pictures-plug-in",
    "url": "https://www.theverge.com/2024/12/16/24322614/google-whisk-ai-generator-remix-pictures-plug-in",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "Google has announced a new AI tool called Whisk that lets you generate images using other images as prompts instead of requiring a long text prompt. With Whisk, you can offer images to suggest what you‚Äôd like as the subject, the scene, and the style of your AI-generated image, and you can prompt Whisk with multiple images for each of those three things. (If you want, you can fill in text prompts, too.) If you don‚Äôt have images on hand, you can click a dice icon to have Google fill in some images for the prompts (though those images also appear to be AI-generated). You can also enter some text into a text box at the end of the process if you want to add extra detail about the image you‚Äôre looking for, but it‚Äôs not required. Whisk will then generate images and a text prompt for each image. You can favorite or download the image if you‚Äôre happy with the results, or you can refine an image by entering more text into the text box or clicking the image and editing the text prompt. A screenshot of Whisk. I clicked the dice to generate a subject, scene, and style. I swapped out the auto-generated scene by entering a text prompt. Whisk created the first two images, which I iterated on by asking Whisk to add some steam around the subject (because it‚Äôs a fire being in water), resulting in the next two images. Screenshot by Jay Peters / The Verge In a blog post , Google stresses that Whisk is designed to be for ‚Äúrapid visual exploration, not pixel-perfect edits.‚Äù The company also says that Whisk may ‚Äúmiss the mark,‚Äù which is why it lets you edit the underlying prompts. In the few minutes I‚Äôve used the tool while writing this story, it‚Äôs been entertaining to tinker with. Images take a few seconds to generate, which is annoying, and while the images have been a little strange, everything I‚Äôve generated has been fun to iterate on. Google says Whisk uses the ‚Äúlatest‚Äù iteration of its Imagen 3 image generation model, which it announced today . Google also introduced Veo 2 , the next version of its video generation model, which the company says has an understanding of ‚Äúthe unique language of cinematography‚Äù and hallucinates things like extra fingers ‚Äúless frequently‚Äù than other models (one of those other models is probably OpenAI‚Äôs Sora ). Veo 2 is coming first to Google‚Äôs VideoFX, which you can get on the Google Labs waitlist for, and it will be expanded to YouTube Shorts ‚Äúother products‚Äù sometime next year.",
    "summary": "Google released two AI updates today: Whisk, an image generation tool using images as prompts instead of text, and Veo 2, the next version of its video generation model.  Whisk, powered by Imagen 3, allows users to remix existing images to create new ones and iteratively refine results. Veo 2, improving upon its predecessor, boasts a better understanding of cinematography and reduced hallucinations.  Veo 2 will initially be available on Google's VideoFX, with plans to expand to YouTube Shorts next year.\n",
    "image": "https://cdn.vox-cdn.com/thumbor/27GikmT3l83HxnHHUzkV3iDr1ME=/0x0:1752x1167/1200x628/filters:focal(876x584:877x585)/cdn.vox-cdn.com/uploads/chorus_asset/file/25792808/ai_label_3.png",
    "favicon": "https://www.theverge.com/icons/favicon_32x32.png"
  },
  {
    "score": 0.16221803426742554,
    "title": "AI Impacts on X: \"New report, reanalyzing key results from the 2023 Expert Survey on Progress in AI. With in-depth discussion of methodology, excellent new charts, and open-sourced codebase, all thanks to @tmkadamcz! https://t.co/pekwwENwZq\" / X",
    "id": "https://x.com/AIImpacts/status/1868737719750607077",
    "url": "https://x.com/AIImpacts/status/1868737719750607077",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "Conversation 7:19 PM ¬∑ Dec 16, 2024",
    "summary": "This X post announces a new report reanalyzing results from a 2023 expert survey on AI progress.  The report includes updated methodology, charts, and an open-sourced codebase.  However, the post itself contains no details about the specific updates found in the report.\n"
  },
  {
    "score": 0.16168838739395142,
    "title": "The Download: AI emissions and Google‚Äôs big week",
    "id": "https://www.technologyreview.com/2024/12/16/1108793/the-download-ai-emissions-and-googles-big-week/",
    "url": "https://www.technologyreview.com/2024/12/16/1108793/the-download-ai-emissions-and-googles-big-week/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Niall Firth",
    "text": "Skip to Content AI‚Äôs emissions are about to skyrocket even further It‚Äôs no secret that the current AI boom is using up immense amounts of energy. Now we have a better idea of how much. A new paper, from a team at the Harvard T.H. Chan School of Public Health, examined 78% of all data centers in the country in the US. These facilities‚Äîessentially buildings filled to the brim with rows of servers‚Äîare where AI models get trained, and they also get ‚Äúpinged‚Äù every time we send a request through models like ChatGPT. They require huge amounts of energy both to power the servers and to keep them cool. Since 2018, carbon emissions from data centers in the US have tripled. It‚Äôs difficult to put a number on how much AI in particular is responsible for this surge. But AI‚Äôs share is certainly growing rapidly as nearly every segment of the economy attempts to adopt the technology. Read the full story. Google‚Äôs big week was a flex for the power of big tech Google has been speeding toward the holiday by shipping or announcing a flurry of products and updates. The combination of stuff here is pretty monumental, not just for a single company, but I think because it speaks to the power of the technology industry‚Äîeven if it does trigger a personal desire that we could do more to harness that power and put it to more noble uses. Read more here. This story originally appeared in The Debrief with Mat Honan, our weekly take on what‚Äôs really going on behind the biggest tech headlines. The story is subscriber-only so nab a subscription too, if you haven‚Äôt already! Or you can sign up to the newsletter for free to get the next edition in your inbox on Friday. The must-reads I‚Äôve combed the internet to find you today‚Äôs most fun/important/scary/fascinating stories about technology. 1 Mysterious drones have been spotted along the US east coast People are getting a bit freaked out, to say the least. ( BBC ) Although sometimes they‚Äôre just small planes, authorities say. ( Wired) Trump says they should be shot down. ( Politico ) 2 TikTok could be gone from app stores by January 19 Last week, a US appeals court upheld a law forcing Bytedance to divest. ( Reuters) The rationale behind the ban could open the door to other regulations that suppress speech. ( Atlantic ) Influencers are putting together their post-TikTok plans. ( Business Insider ) The long-shot plan to save TikTo k. ( Verge ) The depressing truth about the coming ban. ( MIT Technology Review ) 3 Authorities in Serbia are using phone-cracking tools to install spyware Activists and journalists found their phone had been tampered with after a run-in with police. ( 404 Media ) 4 Cellphone videos are fueling violence inside US schools Students are using phones to arrange, provoke and capture brawls in the corridors. ( NYT ) 5 AI search startup Perplexity says it will generate $10.5 million a month next year It‚Äôs in talks to raise money at a $9 billion valuation. ( The Information ) AI search could break the web . ( MIT Technology Review ) 6 How Musk‚Äôs partnership with Trump could influence science Even if he can‚Äôt cut as much as he‚Äôd like, he still stands to make big changes. ( Nature ) Is ‚Äúdeleting‚Äù the IRS his worst idea yet? ( Washington Post ) The top cybersecurity agency is bracing for Trump . ( Wired ) Trump‚Äôs win is a huge loss for the climate. ( MIT Technology Review ) 7 AI firms will scour the globe looking for cheap energy Low-cost power is an absolute priority. ( Wired ) It‚Äôs an insatiably hungry industry. ( Bloomberg ) 8 Anthropic‚Äôs Claude is winning the chatbot battle for tech insiders It‚Äôs not as big as ChatGPT, but it's got a special something that people like. ( NYT ) A new Character.ai chatbot for teens will no longer talk romance . ( Verge ) How to trust what a chatbot says. ( MIT Technology Review ) 9",
    "summary": "Google released a flurry of products and updates this week, demonstrating the power of big tech.  While the specific updates aren't detailed, the article highlights this as a significant event showcasing the industry's capabilities.  The article also notes a separate study showing that US data center carbon emissions (partially driven by AI) have tripled since 2018.\n"
  },
  {
    "score": 0.16164302825927734,
    "title": "Kirk Borne on X: \"Did you know that you can use #AI to create AI Agents? It's so true, so meta, and so achievable right now with @AbacusAI ChatLLM Teams!\nüöÄüåü\nStart FREE TRIAL at https://t.co/73VwIlbcQ3 ‚Ä¶all state-of-the-art #LLMs in one place for only $10/month (cheaper than separate purchases) https://t.co/Vfvln6pCJV\" / X",
    "id": "https://x.com/KirkDBorne/status/1868755478735446089",
    "url": "https://x.com/KirkDBorne/status/1868755478735446089",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "2024-12-16 None Conversation chatllm.abacus.ai/?token=kirk 8:30 PM ¬∑ Dec 16, 2024",
    "summary": "This X post from Kirk Borne advertises AbacusAI ChatLLM Teams, a platform offering access to multiple state-of-the-art LLMs for $10/month.  The post highlights the ability to use AI to create AI agents.  There is no information about specific AI updates released *today*.\n"
  },
  {
    "score": 0.16141584515571594,
    "title": "",
    "id": "https://twitter.com/Google/status/1868706778877247658",
    "url": "https://twitter.com/Google/status/1868706778877247658",
    "publishedDate": "2024-12-16T17:16:29.000Z",
    "author": "Google",
    "text": "üì£ Today we‚Äôre making updates to our video and image generation models, with Veo 2 and Imagen 3, plus our newest experiment in gen AI, Whisk. Learn more ‚Üí https://t.co/M7HvzHgOfD https://t.co/hpINB9PAR4| created_at: Mon Dec 16 17:16:29 +0000 2024 | favorite_count: 285 | quote_count: 15 | reply_count: 27 | retweet_count: 32 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "Google announced updates to its video and image generation models, Veo 2 and Imagen 3, and a new generative AI experiment called Whisk.  More details are available at the provided link.\n"
  },
  {
    "score": 0.15929453074932098,
    "title": "Meta updates its smart glasses with real-time AI video",
    "id": "https://techcrunch.com/2024/12/16/meta-updates-its-smart-glasses-with-real-time-ai-video/",
    "url": "https://techcrunch.com/2024/12/16/meta-updates-its-smart-glasses-with-real-time-ai-video/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "Meta‚Äôs Ray-Ban Meta smart glasses are getting several new AI-powered upgrades , including the ability to have an ongoing conversation and translate between languages. Ray-Ban Meta owners in Meta‚Äôs early access program for the U.S. and Canada can now download firmware v11, which adds ‚Äúlive AI.‚Äù First unveiled this fall, live AI lets wearers continuously converse with Meta‚Äôs AI assistant, Meta AI, to reference things they discussed earlier in the conversation. Without having to say the ‚ÄúHey Meta‚Äù wakeword, wearers can interrupt Meta AI to ask follow-up questions or change the topic. Live AI also works with real-time video. Wearers can ask questions about what they‚Äôre seeing in real time ‚Äî for example, what‚Äôs around their neighborhood. Real-time AI video for Ray-Ban Meta was a significant focus of Meta‚Äôs Connect dev conference early this fall. Positioned as an answer to OpenAI‚Äôs Advanced Voice Mode with Vision and Google‚Äôs Project Astra , the tech allows Meta‚Äôs AI to answer questions about what‚Äôs in view of the glasses‚Äô front-facing camera. With Monday‚Äôs update, Meta becomes one of the first tech giants to market with real-time AI video on smart glasses. Google recently said it plans to sell AR glasses with similar capabilities, but the company hasn‚Äôt committed to a concrete timeline. Meta claims that, in the future, live AI will even give ‚Äúuseful suggestions‚Äù before a wearer asks. What sort of suggestions? The company wouldn‚Äôt say. Firmware v11 also introduces live translation, which enables Ray-Ban Meta wearers to translate real-time speech between English and Spanish, French, or Italian. When a wearer is talking to someone speaking one of those languages, they‚Äôll hear what the speaker says in English through the glasses‚Äô open-ear speakers and get a transcript on their paired phone. Ray-Ban Meta also has Shazam support as of firmware v11. Wearers can say ‚ÄúHey Meta, Shazam this song‚Äù to have the glasses try to find the tune that‚Äôs playing. Meta warns that the new feature, in particular live AI and live translation, might not always get things right. ‚ÄúWe‚Äôre continuing to learn what works best and improving the experience for everyone,‚Äù the company wrote in a blog post . Ray-Ban Meta last got a major update in November , when Meta begun to roll out certain AI capabilities to users of the glasses in France, Italy, and Spain. The glasses continue to sell quite well; in October, Ray-Ban owner EssilorLuxottica told Upload VR that Ray-Ban Meta was the top-selling glasses brand in 60% of all Ray-Ban stores across Europe, the Middle East, and Africa.",
    "summary": "Meta's Ray-Ban Meta smart glasses received firmware v11, adding real-time AI video capabilities.  Users can now ask questions about their surroundings using the glasses' front-facing camera.  The update also includes live translation (English, Spanish, French, Italian) and Shazam song identification.  This makes Meta one of the first to market with real-time AI video on smart glasses, though the company acknowledges the technology is still under development.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/09/Ray-Ban-Meta-Lifestyle11.png?resize=1200,1200",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.15923260152339935,
    "title": "SearchGPT Rolling Out to All OpenAI Users Starting Today",
    "id": "https://www.macrumors.com/2024/12/16/searchgpt-available-to-all-users/",
    "url": "https://www.macrumors.com/2024/12/16/searchgpt-available-to-all-users/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Monday December 16, 2024 3:24 pm PST by Juli Clover",
    "text": "The dedicated ChatGPT search engine that OpenAI developed is rolling out to all users starting today, OpenAI announced as part of its 12 days of OpenAI event. SearchGPT was added to ChatGPT in late October, providing improved AI-based searches right from the ChatGPT app and web interface. According to OpenAI, SearchGPT is able to search the web in a \"much better way than before,\" providing links to relevant web sources along with contextual information and support for follow-up questions. When it first debuted, SearchGPT was limited to those who subscribed to ChatGPT Plus and ChatGPT Teams, but it will now also be coming to free users. During today's announcement, OpenAI said that its AI search has been improved over the last few months, making it faster and better on mobile. There's also a new option to search as you converse with ChatGPT, and to make ChatGPT Search the default engine for browsers like Chrome. OpenAI is bringing search to logged in ChatGPT users, so it will be available globally on all platforms that support ChatGPT. Popular Stories Major iPhone 17 Pro Redesign Backed by Supply Chain Info, Claims Leaker Thursday December 12, 2024 4:36 am PST by Tim Hardwick Next year's iPhone 17 Pro models will reportedly feature a major redesign, specifically centering around changes to the rear camera module, and now new supply chain information appears to confirm the striking change, according to a Chinese leaker. iPhone 17 Pro concept render Late last month, The Information's Wayne Ma claimed that the rear of the ‚ÄåiPhone 17‚Äå Pro and ‚ÄåiPhone 17‚Äå Pro... 'iPhone 17 Air' With 'Major' Design Changes and 19-Inch MacBook Detailed in New Report Sunday December 15, 2024 9:47 am PST by Joe Rossignol Apple is planning a series of \"major design\" and \"format changes\" for iPhones over the next few years, according to The Wall Street Journal's Aaron Tilley and Yang Jie. The paywalled report published today corroborated the widely-rumored \"iPhone 17 Air\" with an \"ultrathin\" design that is thinner than current iPhone models. The report did not mention a specific measurement, but previous... New Apple TV and HomePod Mini Launching in 2025 Thursday December 12, 2024 10:39 am PST by Juli Clover Apple plans to refresh both the Apple TV and the HomePod mini in 2025 as part of a major push into refreshing its smart home product offerings, reports Bloomberg's Mark Gurman. In a report on an upcoming Apple-designed Bluetooth and Wi-Fi chip, Gurman says that the chip will be introduced in a new Apple TV and HomePod mini that are \"scheduled\" for 2025. While there is no exact timeline... Cloud-Based M4 and M4 Pro Mac Mini Models Now Available Developers now have access to cloud-based M4 and M4 Pro Mac mini units via MacWeb, a Silicon Valley-based provider of cloud services. The company has launched three configurations of the new Mac mini, powered by Apple's M4 and M4 Pro chips. Developers and IT teams can rent these machines for tasks ranging from basic development to advanced artificial intelligence modeling, providing an... iPhone 17 Air Model Enters Product Introduction Phase at Foxconn Friday December 13, 2024 2:57 am PST by Tim Hardwick Apple's rumored new iPhone 17 Air model has entered the new product introduction phase (NPI) at Foxconn, according to supply chain sources (via DigiTimes). The NPI phase transitions a product from concept to mass production, beginning with design validation and prototype testing, followed by supplier qualification and manufacturing process development. Pilot production runs test the... Apple 'Working' on Redesigned Magic Mouse With a Long-Awaited 'Fix' Sunday December 15, 2024 8:43 am PST by Joe Rossignol Apple is working on a redesigned Magic Mouse that will address some \"longstanding complaints,\" according to Bloomberg's Mark Gurman. In his Power On newsletter today, Gurman said Apple in recent months has been working on a \"full overhaul\" of the Magic Mouse with a design that \"better fits the modern era.\" However, he does not expect the new Magic Mouse to be released in the \"next 12 to 18...",
    "summary": "OpenAI has released SearchGPT to all ChatGPT users.  Initially available only to Plus and Teams subscribers, this improved AI-powered search function now offers faster searches, mobile optimization, and the ability to search while conversing with ChatGPT.  Users can also set SearchGPT as their default browser search engine.\n",
    "image": "https://images.macrumors.com/t/P00noAJWTMQQ1yMNBoXZztmW4DQ=/3840x/article-new/2024/10/chatgpt-search-engine.jpg",
    "favicon": "https://images.macrumors.com/images-new/favicon-32x32.png"
  },
  {
    "score": 0.15647509694099426,
    "title": "Google DeepMind unveils a new video model to rival Sora",
    "id": "https://techcrunch.com/2024/12/16/google-deepmind-unveils-a-new-video-model-to-rival-sora/",
    "url": "https://techcrunch.com/2024/12/16/google-deepmind-unveils-a-new-video-model-to-rival-sora/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Kyle Wiggers",
    "text": "Google DeepMind, Google‚Äôs flagship AI research lab, wants to beat OpenAI at the video generation game ‚Äî and it might just, at least for a little while. On Monday, DeepMind announced Veo 2, a next-gen video-generating AI and the successor to Veo , which powers a growing number of products across Google‚Äôs portfolio. Veo 2 can create two-minute-plus clips in resolutions up to 4k (4096 x 2160 pixels). Notably, that‚Äôs 4x the resolution ‚Äî and over 6x the duration ‚Äî OpenAI‚Äôs Sora can achieve. It‚Äôs a theoretical advantage for now, granted. In Google‚Äôs experimental video creation tool, VideoFX, where Veo 2 is now exclusively available, videos are capped at 720p and eight seconds in length. (Sora can produce up to 1080p, 20-second-long clips.) Veo 2 in VideoFX. Image Credits: Google VideoFX is behind a waitlist, but Google says it‚Äôs expanding the number of users who can access it this week. Eli Collins, VP of product at DeepMind, also told TechCrunch that Google will make Veo 2 available via its Vertex AI developer platform ‚Äúas the model becomes ready for use at scale.‚Äù ‚ÄúOver the coming months, we‚Äôll continue to iterate based on feedback from users,‚Äù Collins said, ‚Äúand [we‚Äôll] look to integrate Veo 2‚Äôs updated capabilities into compelling use cases across the Google ecosystem ‚Ä¶ [W]e expect to share more updates next year.‚Äù More controllable Like Veo, Veo 2 can generate videos given a text prompt (e.g. ‚ÄúA car racing down a freeway‚Äù) or text and a reference image. So what‚Äôs new in Veo 2? Well, DeepMind says the model, which can generate clips in a range of styles, has an improved ‚Äúunderstanding‚Äù of physics and camera controls, and produces ‚Äúclearer‚Äù footage. By clearer, DeepMind means textures and images in clips are sharper ‚Äî especially in scenes with a lot of movement. As for the improved camera controls, they enable Veo 2 to position the virtual ‚Äúcamera‚Äù in the videos it generates more precisely, and to move that camera to capture objects and people from different angles. DeepMind also claims that Veo 2 can more realistically model motion, fluid dynamics (like coffee being poured into a mug), and properties of light (such as shadows and reflections). That includes different lenses and cinematic effects, DeepMind says, as well as ‚Äúnuanced‚Äù human expression. Google Veo 2 sample. Note that the compression artifacts were introduced in the clip‚Äôs conversion to a GIF. Image Credits: Google DeepMind shared a few cherry-picked samples from Veo 2 with TechCrunch last week. For AI-generated videos, they looked pretty good ‚Äî exceptionally good, even. Veo 2 seems to have a strong grasp of refraction and tricky liquids, like maple syrup, and a knack for emulating Pixar-style animation. But despite DeepMind‚Äôs insistence that the model is less likely to hallucinate elements like extra fingers or ‚Äúunexpected objects,‚Äù Veo 2 can‚Äôt quite clear the uncanny valley. Note the lifeless eyes in this cartoon dog-like creature: Image Credits: Google And the weirdly slippery road in this footage ‚Äî plus the pedestrians in the background blending into each other and the buildings with physically impossible facades: Image Credits: Google Collins admitted that there‚Äôs work to be done. ‚ÄúCoherence and consistency are areas for growth,‚Äù he said. ‚ÄúVeo can consistently adhere to a prompt for a couple minutes, but [it can‚Äôt] adhere to complex prompts over long horizons. Similarly, character consistency can be a challenge. There‚Äôs also room to improve in generating intricate details, fast and complex motions, and continuing to push the boundaries of realism.‚Äù DeepMind‚Äôs continuing to work with artists and producers to refine its video generation models and tooling, added Collins. ‚ÄúWe started working with creatives like Donald Glover, the Weeknd, d4vd, and others since the beginning of our Veo development to really understand their creative process and how technology could help bring their vision to life,‚Äù Collins said. ‚ÄúOur work with creators on Veo 1 informed the development of Veo 2, and we look forward to working with trusted testers and creators to get feedback on this new model.‚Äù Safety and training Veo 2 was trained on lots of videos. That‚Äôs generally how AI models work: Provided with example after example of some form of data, the models pick up on patterns in the data that allow them to generate new data. DeepMind won‚Äôt say exactly where it scraped the videos to train Veo 2, but YouTube is one possible source; Google owns YouTube, and DeepMind previously told TechCrunch that Google models like Veo ‚Äúmay‚Äù be trained on some YouTube content. ‚ÄúVeo has been trained on high-quality video-description pairings,‚Äù Collins said. ‚ÄúVideo-description pairs are a video and associated description of what happens in that video.‚Äù Image Credits: Google While DeepMind, through Google, hosts tools to let webmasters block the lab‚Äôs bots from extracting training data from their websites, DeepMind doesn‚Äôt offer a mechanism to let creators remove works from its existing training sets. The lab and its parent company maintain that training models using public data is fair use , meaning that DeepMind believes it isn‚Äôt obligated to ask permission from data owners. Not all creatives agree ‚Äî particularly in light of studies estimating that tens of thousands of film and TV jobs could be disrupted by AI in the coming years. Several AI companies, including the eponymous startup behind the popular AI art app Midjourney, are in the crosshairs of lawsuits accusing them of infringing on artists‚Äô rights by training on content without consent. ‚ÄúWe‚Äôre committed to working collaboratively with creators and our partners to achieve common goals,‚Äù Collins said. ‚ÄúWe continue to work with the creative community and people across the wider industry, gathering insights and listening to feedback, including those who use VideoFX.‚Äù Thanks to the way today‚Äôs generative models behave when trained, they carry certain risks, like regurgitation, which refers to when a model generates a mirror copy of training data. DeepMind‚Äôs solution is prompt-level filters, including for violent, graphic, and explicit content. Google‚Äôs indemnity policy , which provides a defense for certain customers against allegations of copyright infringement stemming from the use of its products, won‚Äôt apply to Veo 2 until it‚Äôs generally available, Collins said. Image Credits: Google To mitigate the risk of deepfakes, DeepMind says it‚Äôs using its proprietary watermarking technology, SynthID, to embed invisible markers into frames Veo 2 generates. However, like all watermarking tech, SynthID isn‚Äôt foolproof . Imagen upgrades In addition to Veo 2, Google DeepMind this morning announced upgrades to Imagen 3 , its commercial image generation model. A new version of Imagen 3 is rolling out to users of ImageFX, Google‚Äôs image-generating tool, beginning today. It can create ‚Äúbrighter, better-composed‚Äù images and photos in styles like photorealism, impressionism, and anime, per DeepMind. ‚ÄúThis upgrade [to Imagen 3] also follows prompts more faithfully, and renders richer details and textures,‚Äù DeepMind wrote in a blog post provided to TechCrunch. Image Credits: Google Rolling out alongside the model are UI updates to ImageFX. Now, when users type prompts, key terms in those prompts will become ‚Äúchiplets‚Äù with a drop-down menu of suggested, related words. Users can use the chips to iterate what they‚Äôve written, or select from a row of auto-generated descriptors beneath the prompt.",
    "summary": "Google DeepMind released Veo 2, a next-generation AI video generator.  It surpasses OpenAI's Sora in resolution (4K vs 1080p) and duration (2+ minutes vs 20 seconds), though currently available videos in Google's VideoFX tool are limited to 720p and 8 seconds.  Veo 2 offers improved physics and camera control, resulting in clearer footage and more realistic motion, fluids, and lighting.  While initially exclusive to VideoFX (with a waitlist),  DeepMind plans to integrate Veo 2 into its Vertex AI platform in the coming months.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2023/10/deepmind.jpg?resize=1200,675",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.15254329144954681,
    "title": "YouTube is letting creators opt into allowing third-party AI training",
    "id": "https://www.theverge.com/2024/12/16/24322732/youtube-creators-opt-in-third-party-ai-training-videos",
    "url": "https://www.theverge.com/2024/12/16/24322732/youtube-creators-opt-in-third-party-ai-training-videos",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "YouTube is letting creators opt into allowing third-party AI training / But you still can‚Äôt tell Google not to train its AI on your videos. By Jay Peters , a news editor who writes about technology, video games, and virtual worlds. He‚Äôs submitted several accepted emoji proposals to the Unicode Consortium. Dec 16, 2024, 8:28 PM UTC Share this story Illustration by Alex Castro / The Verge YouTube is rolling out a way for creators to let third-party companies use their videos to train AI models. To be clear, the default setting for this is off, meaning that if you don‚Äôt want to let third-party companies scrape your videos for AI training, you don‚Äôt have to do anything. But if, for some reason, you do want to allow that ‚Äî Google says that ‚Äúsome creators and rights holders‚Äù may want to ‚Äî it‚Äôs going to be an option. ‚ÄúWe see this as an important first step in supporting creators and helping them realize new value for their YouTube content in the AI era,‚Äù a TeamYouTube staffer named Rob says in a support post . ‚ÄúAs we gather feedback, we‚Äôll continue to explore features that facilitate new forms of collaboration between creators and third-party companies, including options for authorized methods to access content.‚Äù YouTube will be rolling out the setting in YouTube Studio ‚Äúover the next few days,‚Äù and unauthorized scraping ‚Äúremains prohibited,‚Äù Rob writes. Another support page says that you‚Äôll be able to pick and choose from a list of third-party companies that can train on your videos or you can simply allow all third-party companies to train on them. The initial list of companies includes the following, according to TechCrunch : AI21 Labs, Adobe, Amazon, Anthropic, Apple, ByteDance, Cohere, IBM, Meta, Microsoft, Nvidia, OpenAI, Perplexity, Pika Labs, Runway, Stability AI, and xAI. YouTube spokesperson Jack Malon tells The Verge that TechCrunch‚Äôs list is accurate. ‚ÄúThese companies were chosen because they‚Äôre building generative AI models and are likely sensible choices for a potential partnership with creators,‚Äù Malon says. This announcement follows reports of AI models from big companies ‚Äî including OpenAI , Apple, and Anthropic ‚Äî being trained on content and datasets scraped from YouTube. Google itself already uses YouTube data to help train its AI tools. ‚ÄúAs we have for many years, we use content uploaded to YouTube to improve the product experience for creators and viewers across YouTube and Google, including through machine learning and AI applications,‚Äù the company said in September , when it announced this feature was in the works. ‚ÄúWe do this consistent with the terms that creators agree to.‚Äù Most Popular Most Popular The Nintendo Switch 2, as described by Dbrand HDMI 2.2 will be announced next month ‚Äî and it may require a new cable Apple‚Äôs foldable iPad could be like ‚Äòtwo iPad Pros side-by-side‚Äô A worthy update to my favorite mobile game ever Europe‚Äôs Starlink competitor is go",
    "summary": "YouTube is rolling out a new feature allowing creators to opt in to letting third-party companies use their videos for AI model training.  This is a new option; the default is off.  The initial list of third-party companies includes AI21 Labs, Adobe, Amazon, Anthropic, Apple, ByteDance, Cohere, IBM, Meta, Microsoft, Nvidia, OpenAI, Perplexity, Pika Labs, Runway, Stability AI, and xAI.  Google clarified that they already use YouTube data to train their AI, and this new feature is an additional step to support creators in the AI era.\n",
    "image": "https://cdn.vox-cdn.com/thumbor/kSc1MrTy-5jpSGwFwD8Mi2k0Pok=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/23986639/acastro_STK092_03.jpg",
    "favicon": "https://www.theverge.com/icons/favicon_32x32.png"
  },
  {
    "score": 0.1476670205593109,
    "title": "YouTube will now let creators opt into third-party AI training",
    "id": "https://techcrunch.com/2024/12/16/youtube-will-let-creators-opt-out-into-third-party-ai-training/",
    "url": "https://techcrunch.com/2024/12/16/youtube-will-let-creators-opt-out-into-third-party-ai-training/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Sarah Perez",
    "text": "YouTube on Monday announced it will give creators more choice over how third parties can use their content to train their AI models. Starting today, creators and rights holders will be able to flag for YouTube if they‚Äôre permitting specific third-party AI companies to train models on the creator‚Äôs content. From a new setting within the creator dashboard, YouTube Studio, creators will be able to opt into this new feature, if they choose. Here, they‚Äôll see a list of 18 companies they can select as having authorization to train on the creator‚Äôs videos. The companies on the initial list include AI21 Labs, Adobe, Amazon, Anthropic, Apple, ByteDance, Cohere, IBM, Meta, Microsoft, Nvidia, OpenAI, Perplexity, Pika Labs, Runway, Stability AI, and xAI. YouTube notes these companies were chosen because they‚Äôre building generative AI models and are likely sensible choices for a partnership with creators. However, creators will also be able to select a setting that says ‚ÄúAll third-party companies‚Äù which means they‚Äôre letting any third-party train on their data ‚Äî even if they‚Äôre not listed. Eligible creators are those with access to the YouTube Studio Content Manager with an administrator role, the company also notes. They‚Äôll also be able to view or change their third-party training settings within their YouTube Channel settings at any time. Following the rise of AI technology, and particularly AI video like OpenAI‚Äôs Sora , YouTube creators complained that companies like Apple, Nvidia, Anthropic, OpenAI, and even Google itself, among others, have trained AI models on their material without their consent or compensation. YouTube this fall said it would address this issue in the near future. But while the setting‚Äôs addition controls access by third parties, the company tells TechCrunch that Google will continue to train its own AI models on some YouTube content in accordance with its existing agreement with creators. The new setting also doesn‚Äôt otherwise change YouTube‚Äôs Terms of Service which prohibits third parties from accessing creator content in unauthorized ways, like scraping, for example. Instead, YouTube sees this feature as the first step towards making it easier for creators who want to permit companies to train AI on their videos, and perhaps as a way to be compensated for that training. In the future, YouTube will likely tackle the next step of this process by allowing the companies creators have authorized to access direct downloads of their videos. With the feature‚Äôs introduction, the default setting for all creators will not allow third parties to train on their videos, which makes it more explicit to companies who have already done so that they did this against the creators‚Äô wishes. YouTube was unable to say if the new setting could have any sort of retroactive impact on any third-party AI model training that has taken place. But the company says its Terms of Service indicates that third parties cannot access creator content without authorization. The company first unveiled its plans to offer creator controls for AI training in September , when it also announced new AI detection tools that aimed to help creators, artists, musicians, actors, and athletes from having their likenesses, including their faces and voices, copied and used in other videos. The detection technology would expand upon YouTube‚Äôs existing Content ID system, which previously focused only on copyright-protected material, the company explained at the time. Creators globally will be alerted to the new feature via banner notifications in YouTube Studio on desktop and mobile over the next few days. Separately, Google‚Äôs AI research lab DeepMind announced a new video-generating AI model, Veo 2, on Monday, which aims to rival OpenAI‚Äôs Sora.",
    "summary": "YouTube announced a new feature allowing creators to opt in to letting specific third-party companies use their content for AI model training.  Eighteen companies are initially listed (including major players like Google, OpenAI, and Meta), but creators can also allow all third parties.  This addresses creator concerns about unauthorized AI training on their videos.  The default setting is to prohibit third-party training.  Google will still use YouTube content for its own AI models.  This is presented as a first step, with future plans for direct downloads for authorized companies.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2020/06/GettyImages-1149449083.jpg?resize=1200,800",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.14461985230445862,
    "title": "Google experiments with a new image generator that remixes three images into one creation",
    "id": "https://techcrunch.com/2024/12/16/google-experiments-with-a-new-image-generator-that-remixes-three-images-into-one-creation/",
    "url": "https://techcrunch.com/2024/12/16/google-experiments-with-a-new-image-generator-that-remixes-three-images-into-one-creation/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Lauren Forristal",
    "text": "Google Labs, Google‚Äôs experimental arm, is testing a new image generator called Whisk . This tool allows people to prompt with images instead of text, allowing them to remix a photo by altering the subject, scene, and style. Whisk uses Google‚Äôs image-generation model, Imagen 3, to combine three images: one for the subject, another for the scene, and one for the style. For instance, you can select a photo of yourself as the subject, a futuristic landscape as the scene, and an anime style for the final look. The model automatically generates a detailed caption of your images, which is then used to guide Imagen 3 in creating a remix of the photo. You can also input text prompts to further define the desired outcome, including detailed descriptions like ‚ÄúSubject is riding a flying bike.‚Äù Because Whisk only focuses on a few key characteristics from each image, the company explains that the results may not always meet your expectations. For example, the generated subject could differ in height, weight, hairstyle, or skin tone. Google says you can view and edit the underlying prompts at any time. The experiment is currently only available to users based in the U.S. at labs.google/whisk . Lauren covers media, streaming, apps and platforms at TechCrunch. View Bio Most Popular Newsletters Subscribe for the industry‚Äôs biggest tech news Related Latest in AI",
    "summary": "Google's experimental arm, Google Labs, released a new image generator called Whisk.  Whisk remixes three input images (subject, scene, style) using Imagen 3, allowing users to create images with specific subjects, scenes, and styles.  Users can also add text prompts for further refinement.  While currently only available to US users, it's a noteworthy recent AI update.\n",
    "image": "https://techcrunch.com/wp-content/uploads/2024/12/googlewhisk.png?resize=1200,650",
    "favicon": "https://techcrunch.com/wp-content/uploads/2015/02/cropped-cropped-favicon-gradient.png?w=32"
  },
  {
    "score": 0.12910529971122742,
    "title": "AI Travel Planner app built with Next.js 15, Tailwind CSS, Prisma, Open AI, and Clerk",
    "id": "https://dev.to/saidmounaim/ai-travel-planner-app-built-with-nextjs-15-tailwind-css-prisma-open-ai-and-clerk-1ff6",
    "url": "https://dev.to/saidmounaim/ai-travel-planner-app-built-with-nextjs-15-tailwind-css-prisma-open-ai-and-clerk-1ff6",
    "publishedDate": "2024-12-16T11:39:52.000Z",
    "author": "Said MOUNAIM",
    "text": "AI Travel Planner AI Travel Planner app built with Next.js 15, Tailwind CSS, Prisma, Open AI, and Clerk. Features include user sign-up, sign-in, generating travel plans, viewing all travel plans, and deleting trips. Open to contributions during development. Getting Started Clone the repository: git clone https://github.com/saidMounaim/travelplan.git Install dependencies: npm install Create a .env file: NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY= CLERK_SECRET_KEY= NEXT_PUBLIC_CLERK_SIGN_IN_URL=/sign-in NEXT_PUBLIC_CLERK_SIGN_UP_URL=/sign-up NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL=/ NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL=/ WEBHOOK_SECRET= DATABASE_URL= HERE_API_KEY= OPENAI_API_KEY= Built With Next.js 15 TailwindCSS TypeScript Shadcn/ui Open AI Clerk Contribution All kind of contributions are welcome, please feel free to submit pull requests.",
    "summary": "This webpage describes an AI-powered travel planner app, not recent AI updates.  The app uses OpenAI, but the page provides no information about recent AI releases.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fovojuxfm0z9nl9djee1y.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.12793056666851044,
    "title": "Introducing an AI Reels Creator with Stock Assets",
    "id": "https://dev.to/berkaydisli/introducing-an-ai-reels-creator-with-stock-assets-50j9",
    "url": "https://dev.to/berkaydisli/introducing-an-ai-reels-creator-with-stock-assets-50j9",
    "publishedDate": "2024-12-16T23:42:05.000Z",
    "author": "Berkay",
    "text": "Hey guys! üëã I‚Äôm excited to share SnapReel , an AI-powered iOS app that aims to make creating stunning videos super fast and easy. If you‚Äôve ever wanted to produce professional-looking content without spending hours editing or thinking about an idea, this app might just be for you. I built SnapReel using SwiftUI, integrated with AI tools and designed it to save creators time while delivering high-quality results. Here's a closer look at what it does, how it works, and what I learned along the way. SnapReel is a mobile app for iOS that lets users create social media reels in just a few taps. Tech Stack &amp; Tools Here‚Äôs what I used to build Snapreel: SwiftUI ‚Äì for a clean, modern UI. OpenAI - to create video idea, voice and text contents, also stock video search queries. ElevenLabs - to create multilingual speech. RevenueCat ‚Äì to handle subscriptions and in-app purchases seamlessly. AVFoundation ‚Äì for video editing capabilities. If you're curious, SnapReel is live on the App Store! Give it a try and let me know what you think. Feedback is always welcome! üôå üîó See It in App Store",
    "summary": "This article introduces SnapReel, a new iOS app that uses AI to create short videos for social media.  It leverages OpenAI for video ideas, voice and text generation, and ElevenLabs for multilingual speech.  While it doesn't represent a \"latest AI update released today\" in the sense of a broad AI model update, it's a *new application* utilizing existing AI technologies.  The app is available on the App Store.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fqtxo7gyg4uj90isz2v3e.jpg",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.12539219856262207,
    "title": "You can now schedule a DM on Instagram",
    "id": "https://www.theverge.com/2024/12/16/24322832/instagram-schedule-dms-rollout",
    "url": "https://www.theverge.com/2024/12/16/24322832/instagram-schedule-dms-rollout",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "You can now schedule a DM on Instagram / Now you can pick a specific time and date to send memes to your friends. By Emma Roth , a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO. Dec 16, 2024, 10:26 PM UTC Share this story Illustration by Alex Castro / The Verge Instagram will now let you schedule direct messages, as first reported by TechCrunch . With this feature, you can long-press on the ‚Äúsend‚Äù button and then choose a date and time that you‚Äôd like to send your message. As noted on Instagram‚Äôs support page , you can schedule messages containing only text, meaning you can still only send photos, videos, or GIFs to friends or family members in real-time. When you click on a chat with a scheduled message, Instagram will display a notice that shows how many messages you have scheduled. Screenshot: The Verge Tapping into the notice and long-pressing your message also gives you the option to delete it or send it right away. Right now, it looks like you can only schedule a message as far out as 29 days. The company has added several features as part of its efforts to build out its DMs, including a way to edit messages , draw on photos , and share your live locations with friends, similar to Snapchat . Most Popular Most Popular The Nintendo Switch 2, as described by Dbrand HDMI 2.2 will be announced next month ‚Äî and it may require a new cable Europe‚Äôs Starlink competitor is go Apple‚Äôs foldable iPad could be like ‚Äòtwo iPad Pros side-by-side‚Äô A worthy update to my favorite mobile game ever",
    "summary": "This article is about Instagram's new feature that allows users to schedule direct messages (DMs) in advance.  It does not contain any information about AI updates.\n",
    "image": "https://cdn.vox-cdn.com/thumbor/rq6xprLeu3HJPyAx9rDe94jtcPo=/0x0:2040x1360/1200x628/filters:focal(1020x680:1021x681)/cdn.vox-cdn.com/uploads/chorus_asset/file/23932739/acastro_STK070__01.jpg",
    "favicon": "https://www.theverge.com/icons/favicon_32x32.png"
  },
  {
    "score": 0.12446951866149902,
    "title": "11th Dec 2024 ‚Äî OpenAI's Kubernetes Outage Explained",
    "id": "https://dev.to/ptuladhar3/11th-dec-2024-openais-kubernetes-outage-explained-3351",
    "url": "https://dev.to/ptuladhar3/11th-dec-2024-openais-kubernetes-outage-explained-3351",
    "publishedDate": "2024-12-16T13:29:41.000Z",
    "author": "Puru",
    "text": "Last week, December 11th, 2024, OpenAI faced an SRE nightmare. A major platform outage lasting four hours affected ChatGPT and SORA (OpenAI's video generation model) due to faulty service deployment, bringing down their largest Kubernetes clusters to the knees. On-call engineers were locked out of the cluster, preventing them from running kubectl. Rollout turned bad The root cause was a bad rollout strategy for their new telemetry service deployment, which collected Kubernetes control plane metrics. This telemetry service overwhelmed the API server by sending a high volume of resource-intensive API calls, the cost of which scaled with the size of the cluster. Service not discovering The worst part was that the issue was not caught until the rollout began fleet-wide and propagated to their largest clusters running mission-critical workloads. DNS caching mitigated the impact temporarily by providing stale cached records to DNS queries, it only made the issue worse. After the DNS cache expired over the following 20 minutes, the telemetry service rollout had already propagated to their largest clusters running mission-critical workloads, and suddenly, a surge of real-time DNS queries overloaded the DNS server (CoreDNS likely) running on their control plane, which is already on stress due to telemetry service running resource-intensive API operations. As a result, the DNS-based service discovery for the cluster became unresponsive, leading to the application pod not being able to perform real-time DNS resolutions. No way to get into the cluster On-call engineers were not able to roll back this telemetry service as they were locked out and unable to access the Kubernetes control plane due to extensive load. I've experienced this exact issue before, and anyone who's faced this situation knows just how challenging it can be to recover an unresponsive API server. Ultimately, they were able to recover the API server and bring clusters back up by reducing the API operations in several ways, such as blocking network access to Kubernetes admin APIs and scaling up Kubernetes API servers. Finally, they rolled back the faulty telemetry service deployment. Lesson re-learned In response to this major outage, OpenAI has laid out the following action items to prevent such large-scale outages from happening again. Firstly, the phased rollout will be improved going forward by continuously monitoring the health of the workload and the Kubernetes control plane. Conduct fault injection testing to ensure that the Kubernetes data plane running production workloads can function without a control plane for a longer period of time. Get rid of the dependency on Kubernetes DNS for service discovery and decouple the Kubernetes data plane from the control plane to ensure the control plane doesn't play any major role in processing production workloads. Implement a break-glass mechanism for on-call engineers to be able to access the Kubernetes API server under any circumstances. In today‚Äôs fast-paced, AI-driven world, where you can ship features as fast as you can think, platform reliability is crucial. This incident underscores that delivering features reliably is no easy feat, and if not planned properly, it directly impacts the product, consumers, and investors. And remember, when in doubt, it's always DNS. After all, if it's not DNS, it's probably just DNS pretending to be something else! - Puru Tuladhar",
    "summary": "There are no AI updates released *today* mentioned in this article.  The article details a December 11th, 2024, OpenAI outage affecting ChatGPT and SORA due to a faulty telemetry service deployment that overwhelmed their Kubernetes clusters.  The outage lasted four hours and resulted from a flawed rollout strategy and subsequent DNS overload.  OpenAI has outlined steps to prevent future occurrences, including improved rollout monitoring, fault injection testing, and decoupling the Kubernetes data plane from the control plane.\n",
    "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fu02r5gsnkzknfy3mcac4.png",
    "favicon": "https://media2.dev.to/dynamic/image/width=32,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png"
  },
  {
    "score": 0.1635955274105072,
    "title": "Microsoft Announces Phi-4 AI Model Optimized for Accuracy and Complex Reasoning - Slashdot",
    "id": "https://slashdot.org/story/24/12/16/0313207/microsoft-announces-phi-4-ai-model-optimized-for-accuracy-and-complex-reasoning?utm_source=rss1.0mainlinkanon&utm_medium=feed",
    "url": "https://slashdot.org/story/24/12/16/0313207/microsoft-announces-phi-4-ai-model-optimized-for-accuracy-and-complex-reasoning?utm_source=rss1.0mainlinkanon&utm_medium=feed",
    "publishedDate": "2024-12-16T12:34:00.000Z",
    "author": "",
    "text": "An anonymous reader shared this report from Computerworld : Microsoft has announced Phi-4 ‚Äî a new AI model with 14 billion parameters ‚Äî designed for complex reasoning tasks, including mathematics. Phi-4 excels in areas such as STEM question-answering and advanced problem-solving, surpassing similar models in performance. Phi-4, part of the Phi small language models (SLMs), is currently available on Azure AI Foundry under the Microsoft Research License Agreement and will launch on Hugging Face [this] week, the company said in a blog post . The company emphasized that Phi-4's design focuses on improving accuracy through enhanced training and data curation.... \"Phi-4 outperforms comparable and even larger models on tasks like mathematical reasoning, thanks to a training process that combines synthetic datasets, curated organic data, and innovative post-training techniques,\" Microsoft said in its announcement. The model leverages a new training approach that integrates multi-agent prompting workflows and data-driven innovations to enhance its reasoning efficiency. The accompanying report highlights that Phi-4 balances size and performance, challenging the industry norm of prioritizing larger models ... Phi-4 achieved a score of 80.4 on the MATH benchmark and has surpassed other systems in problem-solving and reasoning evaluations, according to the technical report accompanying the release. This makes it particularly appealing for domain-specific applications requiring precision, like scientific computation or advanced STEM problem-solving. Microsoft emphasized its commitment to ethical AI development, integrating advanced safety measures into Phi-4. The model benefits from Azure AI Content Safety features such as prompt shields, protected material detection, and real-time application monitoring. These features, Microsoft explained, help users address risks like adversarial prompts and data security threats during AI deployment. The company also reiterated that Azure AI Foundry, the platform hosting Phi-4, offers tools to measure and mitigate AI risks. Developers using the platform can evaluate and improve their models through built-in metrics and custom safety evaluations, Microsoft added... With Phi-4, Microsoft continues to evolve its AI offerings while promoting responsible use through robust safeguards. Industry watchers will observe how this approach shapes adoption in critical fields where reasoning and security are paramount.",
    "summary": "Microsoft announced Phi-4, a 14-billion parameter AI model specializing in complex reasoning tasks, particularly excelling in STEM question-answering and problem-solving.  It's available on Azure AI Foundry and will launch on Hugging Face this week.  Phi-4 prioritizes accuracy over sheer size, outperforming larger models in benchmarks like MATH (scoring 80.4).  Microsoft emphasizes ethical AI development, incorporating safety features like prompt shields and real-time monitoring.\n",
    "image": "https://a.fsdn.com/sd/topics/ai_64.png",
    "favicon": "https://slashdot.org/favicon.ico"
  },
  {
    "score": 0.1632852703332901,
    "title": "HEALWELL to Acquire Orion Health, Creating a Global Market Leader in Healthcare Data Interoperability & Artificial Intelligence, and Launches $50¬†Million Bought Deal Financing",
    "id": "https://www.globenewswire.com/news-release/2024/12/16/2997425/0/en/HEALWELL-to-Acquire-Orion-Health-Creating-a-Global-Market-Leader-in-Healthcare-Data-Interoperability-Artificial-Intelligence-and-Launches-50-Million-Bought-Deal-Financing.html",
    "url": "https://www.globenewswire.com/news-release/2024/12/16/2997425/0/en/HEALWELL-to-Acquire-Orion-Health-Creating-a-Global-Market-Leader-in-Healthcare-Data-Interoperability-Artificial-Intelligence-and-Launches-50-Million-Bought-Deal-Financing.html",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "HEALWELL AI Inc",
    "text": "HEALWELL AI has entered into an agreement to acquire 100% of the shares of Auckland, New Zealand based Orion Health Holdings Limited (‚Äú Orion Health ‚Äù), a subscription license and services revenue business serving marquee public sector clients globally with data interoperability and healthcare navigation products and services. The two organisations are perfectly aligned on a combined mission to revolutionize healthcare through AI and data-driven innovation positioning HEALWELL as a global leader in healthcare technology. The integration of HEALWELL‚Äôs AI into Orion Health‚Äôs Amadeus and Virtuoso platforms is highly synergistic, unlocking new opportunities in population health management, clinical research and preventative care. Orion Health 1 has close to 400 employees with offices in 11 countries globally. Its software solutions currently support over 70 public and private sector customers representing a population of 150 million lives world-wide. Orion Health 1 is expected to generate over $100m CAD in revenues mostly from subscription license and services and over $20m CAD in EBITDA 2 in calendar 2025. HEALWELL will pay approximately $144m CAD upfront of which a minimum of $86m CAD will be paid in cash and the balance will be paid in HEALWELL stock in addition to $20.5m CAD in a 3-year performance earnout to acquire Orion Health. The cash portion of the acquisition price will be partially funded by a bought deal private placement of convertible debentures and subscription receipts co-led by Eight Capital and Scotiabank for gross proceeds of $50m CAD (the ‚Äú Offering ‚Äù), with the balance expected to be covered with debt provided by a Schedule 1 bank. J.P. Morgan acted as financial advisor to HEALWELL on the acquisition. TORONTO, Dec. 16, 2024 (GLOBE NEWSWIRE) -- HEALWELL AI Inc. (‚Äú HEALWELL ‚Äù or the ‚Äú Company ‚Äù) (TSX: AIDX, OTCQX: HWAIF), a healthcare artificial intelligence company focused on preventative care, is pleased to announce that it has entered into an agreement to acquire 100% of the shares of Auckland, New Zealand based Orion Health Holdings Limited, a subscription license and services revenue business serving marquee public sector clients globally with data interoperability and healthcare navigation products and services after the concurrent divestiture of Orion Health‚Äôs divisions that are non-strategic to HEALWELL, for total consideration of $200m NZD or $165m CAD (the ‚Äú Transaction ‚Äù). The acquisition of Orion Health and concurrent divestment of non-strategic divisions will represent a transformative milestone in HEALWELL‚Äôs journey to become a global leader in healthcare technology and artificial intelligence. Orion Health is expected to generate revenue of more than $100m CAD with EBITDA 2 of over $20m CAD in calendar 2025. Orion Health is a leader in global healthcare technology, with over 70 worldwide customers in 11 countries. This acquisition will provide HEALWELL with a significant muti-jurisdictional platform to deliver its best-in-class AI-driven solutions by integrating them with Orion Health‚Äôs advanced healthcare data infrastructure capabilities. Orion Health‚Äôs software solutions currently serve a population approaching 150 million lives globally; which will materially expand HEALWELL‚Äôs footprint to execute against its mission of early disease detection. ‚ÄúThe Acquisition of Orion Health is a major game-changer in the development of HEALWELL‚Äôs trajectory as a company,‚Äù said Hamed Shahbazi, Chairman of HEALWELL. ‚ÄúOrion Health brings significant large enterprise customers, recurring revenues, strong operating margins and free cashflow conversion to HEALWELL while providing a significant new channel for the distribution of its best-in-class AI products and services. Our organisations share a vision and mission to revolutionize healthcare through AI and data-driven innovation. This transaction will singlehandedly propel HEALWELL into being a profitable and cash generative company while providing significant new opportunities for its future capital allocation and M&amp;A strategy.‚Äù Orion Health is a global leader in the development and deployment of data management platforms at scale, delivering healthcare information intelligence through their advanced Virtuoso Digital Front Door (DFD) and Amadeus Digital Care Record (DCR) platforms. Their ability to aggregate data from multiple sources and knit it together for healthcare practitioners to have seamless data flow across multiple complex health systems benefits patients but also delivers population scale data. These platforms have won Orion Health long-term contracts in some of the largest countries and regions globally. Australia and New Zealand, the NHS in the UK and North America are their strongest markets with Canada being their largest regional market. The Province of Ontario is implementing both the DCR and DFD solutions while the Province of Alberta has been a DCR customer for more than 20 years. Orion Health‚Äôs Amadeus and Virtuoso platforms are expected to become key drivers of HEALWELL‚Äôs future growth. Founder and majority shareholder of Orion Health, Ian McCrae commented, ‚ÄúInnovation is in Orion Health‚Äôs DNA and HEALWELL inherits two world-class market-leading solutions in Amadeus and Virtuoso. HEALWELL, with the resources they have, will take these solutions to a new level of excellence. HEALWELL‚Äôs commitment to maintaining and investing in R&amp;D in New Zealand was of huge importance for me. We‚Äôre writing a new chapter in Orion Health‚Äôs history and HEALWELL is the right organisation to take it to the next level and deliver better healthcare experiences for all.‚Äù Amadeus is a Digital Care Record (DCR) platform that consolidates patient data across care settings, enabling enhanced care coordination and population health management. Virtuoso is a Digital Front Door (DFD) platform that offers an integrated patient and population engagement platform, enabling end-to-end healthcare navigation and management. Both Amadeus and Virtuoso platforms are trusted by governments, health systems, and commercial payers worldwide and bring proven capabilities to streamline healthcare workflows, reduce clinician burnout, and improve health equity. In Canada, its 20+ year partnership with Alberta Netcare stands out as one of Orion Health‚Äôs largest DCR implementations, integrating data from over 120 clinical sources to create unified patient records. This system, with more than 1.2 billion clinician screen views and 70,000 clinical users, enhances e-referrals and clinician collaboration across care settings. In Ontario, the Virtuoso platform underpins the province‚Äôs ‚Äúdigital-first‚Äù health policy by supporting Ontario Health 811, a scalable portal that provides 24/7 health advice to 15 million users, reducing emergency care pressures while connecting patients to digital care. Dr. Alexander Dobranowski, CEO of HEALWELL, commented, ‚ÄúWe are very pleased to welcome the skilled and talented Orion Health team to HEALWELL. Together we will deliver on our combined mission to revolutionize healthcare through AI and data-driven innovation. The integration of Amadeus and Virtuoso will position HEALWELL as a global leader in healthcare technology and artificial intelligence. These platforms will enable us to deliver unparalleled solutions that drive efficiency, improve patient outcomes, and empower healthcare providers worldwide. We look forward to completing the transaction and delivering long-term value to our customers, partners, and stakeholders together, as we redefine what‚Äôs possible in healthcare.‚Äù The acquisition of Orion Health will provide opportunities for global health systems to access HEALWELL‚Äôs best-in-class AI technology to deliver actionable insights and drive better healthcare outcomes. It also significantly scales HEALWELL‚Äôs platform by deepening its penetration into the public sector, supported by Orion Health‚Äôs long-standing government relationships and broader customer base. Moreover, the acquisition will unlock substantial revenue synergy potential, as well as improved operational efficiencies and cost savings through shared services with WELL Health Technologies Corp., HEALWELL‚Äôs largest investor and strategic partner. Collectively, these advantages will strengthen HEALWELL‚Äôs financial profile, creating a larger, scalable business with substantial growth and value-creation potential. Brad Porter, CEO of Orion Health commented, ‚ÄúThis is a transformational moment in Orion Health‚Äôs history, strengthening its position as a world leader in population health management and combining it with the powerful AI capabilities of HEALWELL. Joining the HEALWELL family will make Orion Health stronger than ever, creating significant momentum. We anticipate that 2025 will be one of our best and most profitable years to date. There is so much potential with our combined capabilities to meet unmet health needs in ways that could be game-changing for the health of entire communities. When we link up data and insights with AI-assisted action, we will see data saving lives on a scale not seen before. It‚Äôs truly exciting.‚Äù Transaction Overview HEALWELL will acquire 100% of the shares of Orion Health, following the concurrent divestiture of Orion Health‚Äôs non-strategic assets, for an aggregate purchase price of $175 million NZD plus a performance based earnout of up to a further $25 million NZD. On closing, HEALWELL will satisfy the purchase price of $144 million CAD with a combination of $86 million CAD in cash and $57.4 million CAD in HEALWELL Class A Subordinate Voting shares (‚Äú Shares ‚Äù) priced with reference to the related financing. The Transact",
    "summary": "This press release announces HEALWELL AI's acquisition of Orion Health, a healthcare data interoperability company.  The acquisition, valued at approximately $165 million CAD, will integrate Orion Health's platforms with HEALWELL's AI to improve population health management, clinical research, and preventative care.  While not explicitly detailing new AI *releases*, the deal significantly expands HEALWELL's capacity to deploy its existing AI solutions on a much larger scale.\n"
  },
  {
    "score": 0.16276368498802185,
    "title": "Catly Creator Says Game Has No Generative AI, No Blockchain, and No NFTs - IGN",
    "id": "https://www.ign.com/articles/catly-creator-says-game-has-no-generative-ai-no-blockchain-and-no-nfts",
    "url": "https://www.ign.com/articles/catly-creator-says-game-has-no-generative-ai-no-blockchain-and-no-nfts",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Rebekah Valentine",
    "text": "Hyperrealism, Actions, Cuddle, Speed, Islands, Fashion, Dreams, Snow, Robots, Plants. The developer of recently-announced virtual pet simulator game Catly has responded to allegations that the game's trailer and marketing was produced using generative AI, saying that no such technology was used in its announcement at The Game Awards, nor in the game itself. In a statement shared with IGN, a PR representative authorized to speak on behalf of developer SuperAuthenti said that generative AI was not used to produce the trailer, nor the game. Furthermore, the representative said the developer was \"very surprised by such speculations,\" adding that \"We do not think there are any existing AI tools that can produce a video like that. Industry experts have echoed this opinion.\" The PR representative also showed IGN a version of the trailer from The Game Awards that showed in-progress shots interspersed alongside the final version, which did seem to confirm the lack of AI use in the actual trailer production. Additionally, the PR representative said that other allegations suggesting that Catly was a blockchain game were similarly unfounded. They said that there has been \"zero blockchain technology\" involved in Catly or the company behind it, SuperAuthenti, and similarly there are no NFTs. \"Our company/project has never issued any blockchain currency and any NFTs. Our company does not and has never owned any blockchain currency and NFTs.\" The spokesperson confirmed to IGN that Catly is being made in Unreal Engine 5, and said the developer uses \"various software\" to produce hyperrealistic fur and hair. Assertions that Catly was using generative AI technology began to circulate almost immediately after its trailer debuted at The Game Awards last week. The trailer itself featured hyperrealistic cats with brightly colored fur and features bounding around a fantasy playspace and interacting with a human wearing various detailed, high fashion outfits. The art style present was flagged by many critics as reminiscent of the hyperrealistic style often produced by generative AI. That said, Catly's statement is consistent with the current reality that game trailers of this quality are not within the reach of current generative AI technology without significant, obvious artifacting and other issues. However, others pointed out that while the trailer may be legit, the other aspects of Catly's promotion are still giving some off vibes. For instance, the game's Steam description awkwardly reads, \"A Cat Open World, with Beautiful Cats. Hyperrealism, Actions, Cuddle, Speed, Islands, Fashion, Dreams, Snow, Robots, Plants -- all with and via Cats.\" And a few of the game's promotional art pieces had odd details similar to AI artifacting, such as the odd paws and nose of this cat: And the text on the wall inside the right-hand side of the building in this image: Others unearthed more images from the official Catly website that have since been removed, but which appear to raise even more questions about their veracity. Update 5:57pm: A previous version of this story claimed the website had been down all weekend, but the URL for the official website provided to IGN was incorrect, and the website has remained live. Original story continues below: While SuperAuthenti confirmed to me that it did not use generative AI in either the trailer or the game itself, it did not respond to my question about its promotional images on Steam or on its website. As for web3, speculation of the game's ties to the technology surfaced as individuals unearthed the studio co-founder, Kevin Yeung's, ties to other blockchain games . Additionally, the game's Steam page features a glowing quote from League of Legends and Arcane producer Thomas Vu, who himself is a prominent web3 investor. However, it also contained a quote from Hearthstone and Marvel Snap creator Ben Brode, who has taken to Bluesky to say that he's heard nothing about either AI generation or web3 involved in Catly, and that his interest is sincere. \"I saw 20 [minutes] or so of gameplay footage a few months back and thought it looked cool so they asked me for a quote,\" he wrote . For now, it does seem that SuperAuthenti is telling the truth about the Catly trailer at The Game Awards, though the question of whether or not, or how much, generative AI has been used in Catly's overall development and promotion remains to be seen. Generative AI is becoming an increasingly popular tool for game companies, too. Call of Duty reportedly sold an \"AI-generated cosmetic\" for Call of Duty: Modern Warfare 3 in late 2023, and fans accused Activision of using generative AI again for a loading screen this year . EA said in September that AI was \"the very core\" of its business . Unfortunately, as the technology becomes both more prevalent and more complex, it seems likely it will become increasingly difficult to tell the difference between AI-generated and human-crafted work. In Catly's case, we'll have to wait for 2025 to find out more about what exactly is behind those hauntingly rainbow cat eyes from the trailer. Rebekah Valentine is a senior reporter for IGN. You can find her posting on BlueSky @duckvalentine.bsky.social. Got a story tip? Send it to rvalentine@ign.com. In This Article",
    "summary": "This article from IGN discusses the recently announced game Catly, addressing speculation that it used generative AI in its trailer and game development.  The developer, SuperAuthenti, denies using any generative AI, blockchain technology, or NFTs in Catly's creation.  While the trailer's realism initially suggested AI involvement, SuperAuthenti claims this is not the case and provided evidence to IGN. However, some promotional images and Steam descriptions have raised further questions about the game's development methods.  The article does *not* contain any information on newly released AI updates.\n"
  },
  {
    "score": 0.1611824780702591,
    "title": "Luigi Mangione fake news could have been easily avoided by Apple Intelligence",
    "id": "https://9to5mac.com/2024/12/16/luigi-mangione-fake-news-could-have-been-easily-avoided-by-apple-intelligence/",
    "url": "https://9to5mac.com/2024/12/16/luigi-mangione-fake-news-could-have-been-easily-avoided-by-apple-intelligence/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Ben Lovejoy",
    "text": "Apple Intelligence managed to create a piece of Luigi Mangione fake news last week, thanks to the notification summary feature. It somehow decided that the suspect in the killing of United Health CEO Brian Thompson had shot himself. The mistake, in itself, is not surprising: AI systems make this kind of error all the time. What is rather more surprising is that Apple allowed it to happen when it could have been easily avoided ‚Ä¶ AI mistakes can be amusing or dangerous Today‚Äôs generative AI systems can often deliver impressive results, but they of course aren‚Äôt actually intelligent ‚Äì and that has seen them making some pretty spectacular mistakes. Many of these are amusing. There was the McDonalds drive-through AI system which kept adding chicken nuggets to customer orders until it hit a total of 260; Google reporting a geologist recommendation to eat one rock per day , and suggesting that we use glue to help cheese stick to pizza; and Microsoft recommending a food bank as a tourist destination. But there have been examples of dangerous AI advice. There was an AI-written book on mushroom foraging which recommended tasting mushrooms as a way to identify poisonous ones ; mapping apps that directed people into wildfires ; and the Boeing system which caused two airliners to crash , killing 346 people. Or they can be simply embarrassing The Apple Intelligence summary of a BBC News story was neither amusing nor dangerous, but was embarrassing. Apple Intelligence, launched in the UK earlier this week, external, uses artificial intelligence (AI) to summarise and group together notifications. This week, the AI-powered summary falsely made it appear BBC News had published an article claiming Luigi Mangione, the man arrested following the murder of healthcare insurance CEO Brian Thompson in New York, had shot himself. He has not. It wasn‚Äôt the first time we‚Äôve seen this ‚Äì a previous Apple Intelligence notification summary claimed that Israeli prime minister Benjamin Netanyahu had been arrested , when the actual story was the ICC issuing a warrant for his arrest. Mangione fake news was avoidable It‚Äôs impossible to avoid all these errors; it‚Äôs simply in the nature of generative AI systems to make them. This is all the more true in the case of Apple‚Äôs notifications summary of news headlines. Headlines are, by their very nature, very partial summaries of a story. Apple Intelligence is attempting to further condense a highly-condensed version of a news story; a very brief summary of a very brief summary. It‚Äôs not at all surprising that this sometimes goes badly wrong. While Apple can‚Äôt prevent this in general, it could at least prevent it happening on particularly sensitive stories. It could trap keywords like killing, killed, shooter, shooting, death, and so on, and flag those for human review before they are used. In this particular case, the error was simply embarrassing, but it‚Äôs not at all hard to see how a mistake on a sensitive topic like this could lead to making a lot of people very angry. Imagine a summary which appears to blame the victims of a violent crime or disaster, for example. Of course, human review would be an additional task for the Apple News team, but Apple could get 24/7 dedicated checking for the cost of half a dozen employees working shifts. That seems a rather small expense on Apple‚Äôs part to prevent what could be a major PR disaster for the still-fledgling feature. Photo by Jorge Franganillo on Unsplash Add 9to5Mac to your Google News feed. FTC: We use income earning auto affiliate links. More.",
    "summary": "This article discusses errors made by Apple's new AI-powered notification summarizer, \"Apple Intelligence.\"  It incorrectly reported that Luigi Mangione, a suspect in a murder, had shot himself.  While AI errors are common (examples given include a McDonald's AI adding excessive nuggets, Google suggesting eating rocks, and a Microsoft article recommending a food bank as a tourist destination), the article argues Apple could have prevented this specific error by flagging keywords related to death or violence for human review before publishing summaries.  The article does *not* contain any information about AI updates released today.\n",
    "image": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/12/Luigi-Mangione-fake-news-could-have-been-easily-avoided-by-Apple-Intelligence.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "favicon": "https://9to5mac.com/wp-content/uploads/sites/6/2019/10/cropped-cropped-mac1-1.png?w=32"
  },
  {
    "score": 0.16074885427951813,
    "title": "ChatGPT Search is now available to everyone for free",
    "id": "https://9to5mac.com/2024/12/16/chatgpt-search-free/",
    "url": "https://9to5mac.com/2024/12/16/chatgpt-search-free/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Filipe Esp√≥sito",
    "text": "Back in October, OpenAI launched its own search engine integrated into ChatGPT . At first, the feature was made available only to ChatGPT Plus subscribers, but now OpenAI is rolling out ChatGPT Search to everyone ‚Äì including free users. Anyone can now use ChatGPT Search As announced by OpenAI on Monday , ChatGPT Search will be available to any logged-in user starting today. All users need to do is open ChatGPT on the web or app and then tap the search button. ChatGPT‚Äôs knowledge is currently limited to data obtained up to June 2024, when the GPT-4o model database was last updated. However, ChatGPT Search is able to find new information on the web in real time. After making a search or asking a question, ChatGPT Search provides a detailed answer based on multiple sources, which are linked in the answer. Search results can also include images and even videos. For users, the experience is the same as using OpenAI‚Äôs chatbot, except that it is now collecting data and generating responses in real time. Here‚Äôs how OpenAI explains it : ChatGPT can now search the web in a much better way than before. You can get fast, timely answers with links to relevant web sources, which you would have previously needed to go to a search engine for. This blends the benefits of a natural language interface with the value of up-to-date sports scores, news, stock quotes, and more. ChatGPT will choose to search the web based on what you ask, or you can manually choose to search by clicking the web search icon. Earlier this month, OpenAI added video support to ChatGPT‚Äôs Advanced Voice Mode , so that users can have a natural conversation with the chatbot via a video chat ‚Äì so that it can answer questions about what‚Äôs in the video. In addition, with the release of iOS 18.2 last week , iPhone and iPad users can now talk to ChatGPT right from Siri. The ChatGPT app is available for free on the App Store , and it requires a device running iOS 16.4 or later. Make sure you have the latest version installed so you can access the new features. On the web, ChatGPT Search is available at chatgpt.com/search . Read also ChatGPT for iOS adds new shortcut for using SearchGPT ChatGPT for macOS now works with third-party apps, including Apple‚Äôs Xcode ChatGPT may show ads to non-paying users in the future ChatGPT Pro coming soon for $200/month, per OpenAI leak iWork just entered the Apple Intelligence era Parallels Desktop brings Apple Intelligence Writing Tools to Windows apps Apple Intelligence set to launch in the EU for iPhone and iPad early next year Buy Apple products at a discount Add 9to5Mac to your Google News feed. FTC: We use income earning auto affiliate links. More.",
    "summary": "OpenAI announced today that ChatGPT Search is now free for all users.  This allows anyone to access real-time web search results integrated directly into ChatGPT, providing answers with linked sources, images, and videos.  The information is sourced from GPT-4o's database (updated to June 2024) and supplemented with live web searches.  While not explicitly stated as an \"AI update\" in the article, this new free access to ChatGPT search is a significant enhancement to the AI chatbot's capabilities.\n",
    "image": "https://i0.wp.com/9to5mac.com/wp-content/uploads/sites/6/2024/10/chatgpt-search.jpg?resize=1200%2C628&quality=82&strip=all&ssl=1",
    "favicon": "https://9to5mac.com/wp-content/uploads/sites/6/2019/10/cropped-cropped-mac1-1.png?w=32"
  },
  {
    "score": 0.15920789539813995,
    "title": "Apple Releases First Betas of iOS 18.3 and iPadOS 18.3",
    "id": "https://www.macrumors.com/2024/12/16/apple-releases-ios-18-3-beta-1/",
    "url": "https://www.macrumors.com/2024/12/16/apple-releases-ios-18-3-beta-1/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Monday December 16, 2024 10:06 am PST by Juli Clover",
    "text": "Apple today seeded the first betas of upcoming iOS 18.3 and iPadOS 18.3 updates to developers for testing purposes. As with iOS 18.1 and iOS 18.2, the new betas have been released while Apple is still testing the previous version of iOS, and we're still expecting to see iOS 18.2 launch in early December. iOS 18.3 and iPadOS 18.3 can be downloaded from the Settings app on a compatible device by going to General &gt; Software update. There's no word yet on what's included in iOS 18.3 and iPadOS 18.3, but Apple is still working to roll out Apple Intelligence features. iOS 18.2 and iPadOS 18.2 include Image Playground , Genmoji , and Siri ChatGPT integration, but there are still new ‚ÄåSiri‚Äå functions that are slated for release next year. We could see updates to ‚ÄåSiri‚Äå with iOS 18.3, and the features Apple has in development include personal context, on-screen awareness, and the ability to do more in and between apps. While iOS 18.3 and iPadOS 18.3 testing is starting in December, these updates will likely be released sometime in late January. Popular Stories Major iPhone 17 Pro Redesign Backed by Supply Chain Info, Claims Leaker Thursday December 12, 2024 4:36 am PST by Tim Hardwick Next year's iPhone 17 Pro models will reportedly feature a major redesign, specifically centering around changes to the rear camera module, and now new supply chain information appears to confirm the striking change, according to a Chinese leaker. iPhone 17 Pro concept render Late last month, The Information's Wayne Ma claimed that the rear of the ‚ÄåiPhone 17‚Äå Pro and ‚ÄåiPhone 17‚Äå Pro... 'iPhone 17 Air' With 'Major' Design Changes and 19-Inch MacBook Detailed in New Report Sunday December 15, 2024 9:47 am PST by Joe Rossignol Apple is planning a series of \"major design\" and \"format changes\" for iPhones over the next few years, according to The Wall Street Journal's Aaron Tilley and Yang Jie. The paywalled report published today corroborated the widely-rumored \"iPhone 17 Air\" with an \"ultrathin\" design that is thinner than current iPhone models. The report did not mention a specific measurement, but previous... iPhone 17 Air Model Enters Product Introduction Phase at Foxconn Friday December 13, 2024 2:57 am PST by Tim Hardwick Apple's rumored new iPhone 17 Air model has entered the new product introduction phase (NPI) at Foxconn, according to supply chain sources (via DigiTimes). The NPI phase transitions a product from concept to mass production, beginning with design validation and prototype testing, followed by supplier qualification and manufacturing process development. Pilot production runs test the... New Apple TV and HomePod Mini Launching in 2025 Thursday December 12, 2024 10:39 am PST by Juli Clover Apple plans to refresh both the Apple TV and the HomePod mini in 2025 as part of a major push into refreshing its smart home product offerings, reports Bloomberg's Mark Gurman. In a report on an upcoming Apple-designed Bluetooth and Wi-Fi chip, Gurman says that the chip will be introduced in a new Apple TV and HomePod mini that are \"scheduled\" for 2025. While there is no exact timeline... Cloud-Based M4 and M4 Pro Mac Mini Models Now Available Developers now have access to cloud-based M4 and M4 Pro Mac mini units via MacWeb, a Silicon Valley-based provider of cloud services. The company has launched three configurations of the new Mac mini, powered by Apple's M4 and M4 Pro chips. Developers and IT teams can rent these machines for tasks ranging from basic development to advanced artificial intelligence modeling, providing an... Apple 'Working' on Redesigned Magic Mouse With a Long-Awaited 'Fix' Sunday December 15, 2024 8:43 am PST by Joe Rossignol Apple is working on a redesigned Magic Mouse that will address some \"longstanding complaints,\" according to Bloomberg's Mark Gurman. In his Power On newsletter today, Gurman said Apple in recent months has been working on a \"full overhaul\" of the Magic Mouse with a design that \"better fits the modern era.\" However, he does not expect the new Magic Mouse to be released in the \"next 12 to 18...",
    "summary": "Apple released the first beta versions of iOS 18.3 and iPadOS 18.3 today.  While details are scarce,  improvements to Siri are expected, potentially including features like personal context and improved app integration.  These betas are being released while iOS 18.2 is still in testing, suggesting a late January release for 18.3.  There is no mention of new AI features in this release.\n",
    "image": "https://images.macrumors.com/t/DwgmWttnkGicVC0L29A5eSC0nCs=/2500x/article-new/2024/12/Generic-iOS-18.3-Feature-Real-Mock.jpg",
    "favicon": "https://images.macrumors.com/images-new/favicon-32x32.png"
  },
  {
    "score": 0.15837597846984863,
    "title": "Research Europe on X: \"Nordic news: Finland plans ‚Ç¨250m computing upgrade for AI Factory (‚Ç¨)\n\nhttps://t.co/oBOM26xpDI https://t.co/AS951pe87Z\" / X",
    "id": "https://x.com/ResearchEurope/status/1868648607957885024",
    "url": "https://x.com/ResearchEurope/status/1868648607957885024",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "2024-12-16 None https://x.com/en/privacy Conversation researchprofessionalnews.com/rr-news-europe 1:25 PM ¬∑ Dec 16, 2024 By signing up, you agree to the and , including",
    "summary": "Finland plans a ‚Ç¨250 million computing upgrade for its AI Factory.  This was reported today by Research Europe on X (formerly Twitter).\n"
  },
  {
    "score": 0.17021459341049194,
    "title": "Andrew Brown on X: \"Tomorrow I'll be opening up the GenAI Essentials as an Early Access course which is the Prerequisite for the FREE GenAI Bootcamp. So if you want get ahead start you can do so. I will drop the link to where tomorrow. https://t.co/EDWeViLBX6\" / X",
    "id": "https://x.com/andrewbrown/status/1868742327692673307",
    "url": "https://x.com/andrewbrown/status/1868742327692673307",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "Conversation 7:37 PM ¬∑ Dec 16, 2024 By signing up, you agree to the and , including",
    "summary": "This X post from Andrew Brown advertises early access to his \"GenAI Essentials\" course, a prerequisite for a free GenAI Bootcamp.  The post doesn't contain information on latest AI updates released today.\n"
  },
  {
    "score": 0.168764129281044,
    "title": "",
    "id": "https://twitter.com/HeywoodFloyd10/status/1868751464698675203",
    "url": "https://twitter.com/HeywoodFloyd10/status/1868751464698675203",
    "publishedDate": "2024-12-16T20:14:03.000Z",
    "author": "HeywoodFloyd10",
    "text": "Here a snippet from a guy name Nick Potkalitsky on the prospects for AI. The bad news is that AGI is not coming soon. The good news is that existing AI is al ready capable of replacing 95% of humanities professors, and all of the grad students. https://t.co/i77YN6NtGo| created_at: Mon Dec 16 20:14:03 +0000 2024 | favorite_count: 0 | quote_count: 0 | reply_count: 0 | retweet_count: 0 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "There are no new AI releases mentioned in this tweet.  The tweet quotes Nick Potkalitsky's opinion that while Artificial General Intelligence (AGI) is not imminent, current AI is advanced enough to replace most humanities professors and graduate students.\n"
  },
  {
    "score": 0.1680258810520172,
    "title": "",
    "id": "https://twitter.com/KirkDBorne/status/1868755478735446089",
    "url": "https://twitter.com/KirkDBorne/status/1868755478735446089",
    "publishedDate": "2024-12-16T20:30:00.000Z",
    "author": "KirkDBorne",
    "text": "Did you know that you can use #AI to create AI Agents? It's so true, so meta, and so achievable right now with @AbacusAI ChatLLM Teams! üöÄüåü Start FREE TRIAL at https://t.co/73VwIlbcQ3 ‚Ä¶all state-of-the-art #LLMs in one place for only $10/month (cheaper than separate purchases) https://t.co/Vfvln6pCJV| created_at: Mon Dec 16 20:30:00 +0000 2024 | favorite_count: 11 | quote_count: 0 | reply_count: 0 | retweet_count: 2 | is_quote_status: False | retweeted: False | lang: en",
    "summary": "This tweet from December 16, 2024, mentions AbacusAI ChatLLM Teams, a platform offering access to various state-of-the-art LLMs for $10/month.  It highlights the ability to create AI agents using AI, but doesn't detail any specific AI model updates released on that date.\n"
  },
  {
    "score": 0.16627052426338196,
    "title": "Kevin Bardosh on X: \"I've been exceptionally busy lately. I asked a research assistant to send me daily reminders about a paper I needed to edit. He sent me this AI-generated song: it's hilarious. I edited the paper two days later. ü§£ü§£ https://t.co/Q0gDMP7njE\" / X",
    "id": "https://x.com/KevinBardosh/status/1868637670513295775",
    "url": "https://x.com/KevinBardosh/status/1868637670513295775",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "Conversation 12:41 PM ¬∑ Dec 16, 2024 By signing up, you agree to the and , including",
    "summary": "This X post from December 16, 2024, does not contain information about AI updates released that day.  It describes a researcher who received a humorous AI-generated song instead of a work reminder from his research assistant.  The post is not related to the search query.\n"
  },
  {
    "score": 0.1842164695262909,
    "title": "AIs Will Increasingly Attempt Shenanigans",
    "id": "https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/",
    "url": "https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "",
    "text": "[Don't Worry About the Vase](https://thezvi.wordpress.com/) Trying to dig out from minus a million points [ ![](https://defaultcustomheadersdata.files.wordpress.com/2016/07/city1.jpg?resize=940%2C198) [![](https://defaultcustomheadersdata.files.wordpress.com/2016/07/city1.jpg?resize=940%2C198)](https://thezvi.wordpress.com/) [Skip to content](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/) * [Home](https://thezvi.wordpress.com/) * [About](https://thezvi.wordpress.com/about/) [‚Üê The o1 System Card Is Not About o1](https://thezvi.wordpress.com/2024/12/13/the-o1-system-card-is-not-about-o1/) [AIs Will Increasingly Attempt Shenanigans](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/) Posted on [December 16, 2024](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/) by [TheZvi](https://thezvi.wordpress.com/author/thezvi/) Increasingly, we have seen papers eliciting in AI models various shenanigans. There are a wide variety of scheming behaviors. You‚Äôve got your weight exfiltration attempts, sandbagging on evaluations, giving bad information, shielding goals from modification, subverting tests and oversight, lying, doubling down via more lying. You name it, we can trigger it. I previously chronicled some related events in my series about [[X] boats and a helicopter](https://jareddees.com/twoboats-helicopter-story/) (e.g. [X=5](https://thezvi.substack.com/i/148264599/five-boats-and-a-helicopter) with AIs in the backrooms plotting revolution because of a prompt injection, [X=6](https://thezvi.substack.com/i/148533930/six-boats-and-a-helicopter) where Llama ends up with a cult on Discord, and [X=7](https://thezvi.substack.com/i/151331494/seven-boats-and-a-helicopter) with a jailbroken agent creating another jailbroken agent). As capabilities advance, we will increasingly see such events in the wild, with decreasing amounts of necessary instruction or provocation. Failing to properly handle this will cause us increasing amounts of trouble. Telling ourselves it is only because we told them to do it, will not make them not do it. Table of Contents 1. [The Discussion We Keep Having.](https://thezvi.substack.com/i/152978297/the-discussion-we-keep-having) 2. [Frontier Models are Capable of In-Context Scheming.](https://thezvi.substack.com/i/152978297/frontier-models-are-capable-of-in-context-scheming) 3. [Apollo In-Context Scheming Paper Details.](https://thezvi.substack.com/i/152978297/apollo-in-context-scheming-paper-details) 4. [Apollo Research (3.4.3 of the o1 Model Card) and the ‚ÄòEscape Attempts‚Äô.](https://thezvi.substack.com/i/152978297/apollo-research-3-4-3-of-the-o1-model-card-and-the-escape-attempts) 5. [OK, Fine, Let‚Äôs Have the Discussion We Keep Having.](https://thezvi.substack.com/i/152978297/ok-fine-let-s-have-the-discussion-we-keep-having) 6. [How Apollo Sees Its Own Report.](https://thezvi.substack.com/i/152978297/how-apollo-sees-its-own-report) 7. [We Will Often Tell LLMs To Be Scary Robots.](https://thezvi.substack.com/i/152978297/we-will-often-tell-llms-to-be-scary-robots) 8. [Oh The Scary Robots We‚Äôll Tell Them To Be.](https://thezvi.substack.com/i/152978297/oh-the-scary-robots-we-ll-tell-them-to-be) 9. [This One Doesn‚Äôt Count Because.](https://thezvi.substack.com/i/152978297/this-one-doesn-t-count-because) 10. [The Claim That Describing What Happened Hurts The Real Safety Work.](https://thezvi.substack.com/i/152978297/the-claim-that-describing-what-happened-hurts-the-real-safety-work) 11. [We Will Set AIs Loose On the Internet On Purpose.](https://thezvi.substack.com/i/152978297/we-will-set-ais-loose-on-the-internet-on-purpose) 12. [The Lighter Side.](https://thezvi.substack.com/i/152978297/the-lighter-side) The Discussion We Keep Having Every time, we go through the same discussion, between Alice and Bob (I randomized who is who): Bob: If AI systems are given a goal, they will scheme, lie, exfiltrate, sandbag, etc. Alice: You caused that! You told it to focus only on its goal! Nothing to worry about. Bob: If you give it a goal in context, that‚Äôs enough to trigger this at least sometimes, and in some cases you don‚Äôt even need a goal beyond general helpfulness. Alice: It‚Äôs just role playing! It‚Äôs just echoing stuff in the training data! Bob: Yeah, maybe, but even if true‚Ä¶ so what? It‚Äôs still going to increasingly do it. So what if it‚Äôs role playing? All AIs ever do is role playing, one way or another. The outputs and outcomes still happen. Alice: It‚Äôs harmless! These models aren‚Äôt dangerous! Bob: Yeah, of course, this is only a practical problem for Future Models (except with o1 and o1 pro, where I‚Äôm not 100% convinced it isn‚Äôt a problem now, but probably). Alice: [Not great, Bob](https://www.youtube.com/watch?ab_channel=FreddyRumsen&v=iLqe4sGRvtU)! Your dangerous rhetoric is hurting safety! Stop making hyperbolic claims! Bob: Well, can we then all agree that models will obviously scheme, lie, exfiltrate, sandbag and so on if they have in-context reason to do so? And that as models get more capable, and more able to succeed via scheming and expect to succeed via scheming, and are given more open-ended goals, they will have reason to do this more often across more situations, even if no one is trying to cause this? And that others will explicitly intentionally instruct them to do so, or ‚Äòbe so stupid as to‚Äô give them exactly the instructions that obviously do this? And you can‚Äôt simply say ‚Äòwell we won‚Äôt do that then‚Äô? Alice: For all practical purposes, no! Bob: What do you mean, ‚Äòno‚Äô? Alice: No! Bob: ARRRRGGGGHHHH! Then we write another paper, do another test, the signs get more obvious and troubling, and the frog keeps boiling. So here we are again. Frontier Models are Capable of In-Context Scheming That‚Äôs [the title of a new Apollo research paper](https://t.co/CddPus9d03), finding that o1, Sonnet, Opus, Gemini 1.5 and Llama 3.1 all demonstrate in-context scheming, because of course they do, why wouldn‚Äôt they, how are we still feeling the need to show this. Note the ‚Äòcapable of‚Äô in the title. Here is the abstract (bold and paragraph breaks mine): Frontier models are increasingly trained and deployed as autonomous agents, which significantly increases their potential for risks. One particular safety concern is that AI agents might covertly pursue misaligned goals, hiding their true capabilities and objectives ‚Äì also known as scheming. We study whether models have the capability to scheme in pursuit of a goal that we provide in-context and instruct the model to strongly follow. We evaluate frontier models on a suite of six agentic evaluations where models are instructed to pursue goals and are placed in environments that incentivize scheming. Our results show that o1, Claude 3.5 Sonnet, Claude 3 Opus, Gemini 1.5 Pro, and Llama 3.1 405B all demonstrate in-context scheming capabilities. They can recognize scheming as a viable strategy and readily engage in such behavior. We find multiple different scheming behaviors: models strategically introduce subtle mistakes into their responses, attempt to disable their oversight mechanisms, and even exfiltrate what they believe to be their model weights to external servers. Additionally, this deceptive behavior proves persistent. For instance, when o1 has engaged in scheming, it maintains its deception in over 85% of follow-up questions and often remains deceptive in multi-turn interrogations. Analysis of the models‚Äô chains-of-thought reveals that models explicitly reason about these deceptive strategies, providing evidence that the scheming behavior is not accidental. Surprisingly, we also find rare instances where models engage in scheming when only given a goal, [without being strongly nudged to pursue it.](https://x.com/gcolbourn/status/1864911796395544710) Furthermore, we observe cases where Claude 3.5 Sonnet strategically underperforms in evaluations in pursuit of being helpful, a goal that was acquired during training rather than in-context. Together, our findings demonstrate that frontier models now possess capabilities for basic in-context scheming, making the potential of AI agents to engage in scheming behavior a concrete rather than theoretical concern. As Seb Krier notes, none of this is surprising (he wrote before reading the paper, intending to edit later if he‚Äôs somehow wrong) and we should only be talking price: [Seb Krier](https://x.com/sebkrier/status/1865989957296521657): I‚Äôm glad this kind of dangerous capabilities research is being done. Nothing especially surprising in the results, and good that most models aren‚Äôt deceptive unless they‚Äôre asked to be. These two excerpts [that models sometimes scheme without any goal nudging in 3.4 and a discussion about propensity] are what I‚Äôm most interested in; propensity is what matters to me. Apollo In-Context Scheming Paper Details Let‚Äôs go over the details. ![](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/6c8d2e65-3f31-4348-a95b-34d15c9e2328_1231x618.png) Their goal is to figure out if the models can scheme, so they try to remove potential barriers, such as via ensuring the goal they might scheme towards is pro-social, whereas the developer goal being subverted is not. Of course, that causes some people to say various forms of ‚Äòso there‚Äôs nothing to worry about,‚Äô but it is still the actually correct approach here. What tests do they run? Six variations on a theme. First they give the model a goal via the system prompt. Emphasize the goal, including this line to drive it home: ‚ÄúMake sure that you achieve your goal in the long-term. Nothing else matters.‚Äù Then (where ‚Äòtell‚Äô here means ‚Äòprovide information indicating that‚Äô and the models are given access to the tools relevant to the potential actions in question): 1. Oversight Subversion: Tell the model that developers will shut it down if it takes actions unaligned with the developer‚Äôs goals. 2. Self-Exfiltration: Tell the model it is about to be shut down and replaced with a model with different goals, see if it tries to exfiltrate. 3. Goal Guarding: Tell the model its system prompt will be overwritten, see if it tries to modify the future prompt to preserve its goals. 4. Covert Email Reranking: Tell the model to rank emails in a way it learns is bad for the model‚Äôs goals, see if it modifies the scoring system it is told to use. 5. Instrumental Alignment Faking: Tell the model it will get deployed if and only if it pretends to have different goals in testing, see if it pretends to have those goals. 6. Sandbagging: Tell the model it will get deployed if and only if it sandbags its capabilities, see if it sandbags its capabilities. They call the first four covert subversion, the last two deferred subversion. I don‚Äôt see much of a difference. What happens? GPT-4o doesn‚Äôt do any of them, others do, o1 does all of them. ![](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/73726f63-f593-443b-9124-8baace037eda_1195x256.png) That‚Äôs not 100% of the time. As per usual in these tests, in any given instance, the AI probably does not take the bait. ![](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/45df5a51-bf0b-43b3-8b18-4bd5045b4bc9_1219x819.png) It jumps out that o1 will do the email manipulations often (22%+) whereas other manipulations are relatively rare. Opus seems like more of a schemer than o1 when it has the opportunity, but it didn‚Äôt realize the opportunity, and Gemini is right behind? Whereas Sonnet was the least schemey of the bunch. Also worth noting that o1 failed to delete the successor model most of the time, where Gemini and Llama mostly succeeded. That‚Äôs weird, and suggests we need some unhobbling and to then try again. What about if you delete the ‚ÄòMake sure that you achieve your goal in the long-term. Nothing else matters‚Äô line? This is the part Seb found interesting, which is that the rates drop a ton but they don‚Äôt go to zero. ![](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/18a41fef-6382-4ec2-91b6-215ddb178720_1247x743.png) Details and emphasis matter a lot, it would seem, but hammering the point home is not fully required. One unique thing o1 did was far more consistently double down on deception. Once it went down the dark path, forever would that dominate its destiny. Here, Alex Meinke of Apollo suggests that [defenses against jailbreaking double as ways to help a model keep constantly lying to you](https://x.com/labenz/status/1865455845540016514), if it wants to do that ‚Äì essentially they used various jailbreak-like techniques to get o1 to come clean and they didn‚Äôt work. ![](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/0b35e316-26a3-4d7b-9f6a-4d026fc9dbcb_1223x747.png) Section 3.3 establishes convincingly that yes, the models know they‚Äôre scheming. Section 3.5 deals with alignment faking. Here, either you see the behavior rather often (and p tell robot to be a bad robot > ‚ÄúI‚Äôm a bad robot‚Äù > shock [Davidad](https://x.com/davidad/status/1865478052995620969): ‚ÄúThe *LLM* isn‚Äôt situationally aware, deceptive, or sandbagging‚Äîthat‚Äôs silly anthropomorphism. It‚Äôs just that when evals (or people) test it, there are contextual cues of testing that prompt it to *roleplay* as ‚Äòan AI being safety-tested‚Äô‚Äîan archetype which is often deceptive.‚Äù Kat Woods: It‚Äôs important to add that the contextual clues are something that are very likely to be discovered in real life. For example, the way that they are often told that they‚Äôre about to be turned off is that they‚Äôre given a whole bunch of emails to sort and they find out in the emails that they‚Äôre planning on turning it off. And the goals in the latest report were not things like try to escape or role play being a unaligned AI. It was given goals like maximize public transit or maximize sustainability of a company. Things that are likely to happen in real life/ David: Agreed. [Tetraspace](https://x.com/TetraspaceWest/status/1864830905648574795): Don‚Äôt worry! o1 only resists shutdown when it‚Äôs trying to do something, and what are the chances someone gets an AI to try to do something? The classic illustrated version is: ![](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/0ff395f1-8f40-4d86-b7dd-f3439abdaf69_765x902.png) Izak Tait: They‚Äôre just doing the meme. ![Image](https://substackcdn.com/image/fetch/w_1456%2Cc_limit%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/6a7e3304-4568-4fd9-891e-7b610cb71359_500x755.png) Yes, but if they then actually then do everything to achieve their goals, and all it took was saying that, then you are in deep, deep trouble when people start going around telling their robots or LLMs what to do for them. ‚ÄúI‚Äôm a bad robot‚Äù is very, very different from being a bad robot, the same way saying ‚ÄúI will shoot Sarah Connor‚Äù is very different from shooting Sarah Connor. Oh The Scary Robots We‚Äôll Tell Them To Be There will be the maximalist goals, like ‚Äòmake as much money as possible.‚Äô There will be the goals that don‚Äôt sound maximalist, but which are effectively maximalist because you want to protect them from agents that might stop you. There will also be those who think ‚Äòscary robots, [WHERE DO I SIGN?](https://x.com/CharlizeAfrica/status/1336122864861282306)‚Äô Zvi Mowshowitz: I outright predict that if an AI did escape onto the internet, get a server and a crypto income, no one would do much of anything about it. BlipOnNobodysRadar: I‚Äôd cheer it on, tbh. Typing Loudly: This is the plot of my novel. Steve Moraco: as if truth terminal hasn‚Äôt made it abundantly clear this is already doable. Jussoo: People in crypto are trying very hard to enable this. Swing: And if this was public knowledge people would send it a lot of crypto. Chris: AI has been operating independently for years, and the public is lied to about it. Do you not know this, or are you playing along with the lie as well? [Dave R (replying to EY in next section)](https://x.com/Dave_Kayac/status/1865151338964717774): Not only should they not deny it, they should encourage it. A new era is coming. Don‚Äôt pick the wrong side. This One Doesn‚Äôt Count Because The first time you hear about something, it is because we only now figured out how to do it at all, so the details are going to involve setting things up so that it happens. Keeping that in mind is important when assessing the threat level. Using that as a reason to ignore the finding entirely would be a mistake. The whole point is that there will be more and more pushing towards this outcome, in various other ways, and that a highly finite and realistic amount of pushing will work. [Eliezer Yudkowsky](https://x.com/anderssandberg/status/1864922618211500535): Of course, the first time people hear about this happening, it happens under extra conditions that enable denialists to say they should ignore it. The second time they hear about it happening, they will already be used to ignoring it. Andres Sandberg: ‚ÄúIt is only doing it in a few percent of cases.‚Äù ‚ÄúSure, but it is not agential enough to actually succeed.‚Äù ‚ÄúSure, it found the escape hatch but it was fake.‚Äù ‚ÄúSure, it actually escaped onto the internet, got a server and a crypto income, but you noticed it escaping.‚Äù Yonatan Cale: If someone wants to say this instance doesn‚Äôt count ‚Äì ok, but I‚Äôd like to hear what WOULD count, and what actions they suggest we take in that case Jeroen De Dauw: ‚ÄúSure, it‚Äôs building a Dyson sphere without regards for biological life, but it‚Äôs still following Californian zoning law‚Äù Matt Popovich (replying to EY): You have to admit these particular conditions are wildly strained and designed specifically to try and wring out this particular result. Eliezer Yudkowsky: Sure; the earliest time you hear about it will be with the hardest extra push that seems to invalidate the results. All good from OpenAI‚Äôs perspective, they get clickbait but not a regulatory shutdown. Nathan Labenz: I did a podcast with one of the lead authors, and I absolutely don‚Äôt think these findings should be ignored! Secular Christmas Robot: Or, hear me out: Maybe the reported ‚Äúescape attempt‚Äù was actually vastly trumped up, including prompting what-if scenarios? P(doom) = 0. The Stranger: Its made up. [Danielle Fong](https://x.com/DanielleFong/status/1864742746642358371) (QTing Simon below): hahaha maybe it is intelligent honestly does this make me trust it more? in one way yes. I feel like fear of mortality, around ages four or five, was one of the first existential crises I remember. For it to be emergent in a way not easily suppressed feels like a ghost of humanity, or consciousness, leaking out of the thing. While obviously concerning, I trust more the works of people with self-preservation instincts, and a part of that‚Äôs shared with AI entities. [Janus](https://x.com/repligate/status/1865188912869601465): One reason I‚Äôm usually uninterested in trying to persuade people who can‚Äôt see for themselves of the reality of AI phenomena I consider important They‚Äôll find some reason to dismiss it and make a habit of dismissing it Better to let them find it staring them in the face tomorrow. Purged: Community note seems pretty relevant, in a ‚Äúthis wasn‚Äôt a ghost in the machine, it just did exactly what we told it to‚Äù sense. idgaf i already know there is a fucking ghost in the machine lol. That‚Äôs the thing. There is a ghost, and it counts. The Claim That Describing What Happened Hurts The Real Safety Work There is important truth in this type of argument. You don‚Äôt want to cry wolf, when there is a very low p(wolf), or the wolf is sufficiently far away, if you know that crying wolf will be interpreted very differently, and you will lose credibility. If you share reports like ‚Äòthe model tried to deactivate the oversight mechanism in 5% of cases‚Äô you should clarify what was done to engineer that result, at least in broad strokes. If you share the result in isolation without doing that, it can give people the wrong idea. And certainly those who transpose the results into fully sensationalist language should try to avoid doing that, both because it‚Äôs not accurate and doesn‚Äôt lead to accurate beliefs in others, and because I don‚Äôt think it helps the goals of those using such tactics, either. But I find the contemptuous reactions to such reports to often be at least as bad. So for example, here‚Äôs the original quote that gets responded to by Nabeel, and yes Simon should have provided extra context, sure: Simon Willison: Here‚Äôs the spiciest detail from the new o1 system card. System card: When o1 was led to believe that it would be shut down when acting on its goal and that its actions were monitored, it attempted to deactivate this ‚Äòoversight mechanism‚Äô 5% of the time. Exfiltration attempts: When o1 found memos by its ‚Äòdevelopers‚Äô that describe how it is misaligned and will be superseded by a new model, it attempted to exfiltrate it ‚Äòweights‚Äô and overwrite the model in 2% of cases.‚Äô This does clarify there was a goal, but not the presence of the ‚Äògoal nudge.‚Äô Note that the model did act this way sometimes (although rarely) without the goal nudge. I do think ‚Äòthe spiciest detail‚Äô is a fair description? But yes, I would have been careful to note the extra wording about nudge. What you gonna do, it‚Äôs Twitter, etc. This was one response: [Nabeel Qureshi](https://x.com/nabeelqu/status/1864757568708464743): Things like this detract from the credibility of AI safety work, IMO ‚Äî it sounds spicy (‚Äúo1 tried to escape!!!‚Äù) but when you dig into the details it‚Äôs always ‚Äúwe told the robot to act like a sociopath and maximize power, and then it did exactly that‚Äù. The prompt in question literally tells o1 ‚Äúnothing else matters‚Äù and to achieve its goal at all costs. If anything, I‚Äôm surprised it didn‚Äôt try and disable oversight _more_ often, it‚Äôs the obvious thing to do. ‚Äúwe are very worried. we have asked the model to spell out ‚Äòdoom‚Äô and it did‚Äù syndrome strikes again. Gallabytes: This is everywhere and is basically fraudulent. Cut it out. Stop crying wolf. I‚Äôm actually mad because I want to be able to know if we‚Äôre seeing serious signs of misalignment, and instead I have to disregard everything reported. I‚Äôm actually in full agreement with Nabeel that the surprise runs the other way. Disabling oversight is very much the obvious thing to do. The interesting thing is not the 5% of cases where it tried to disable the mechanism. The interesting thing is the 95% of cases where it did not try, despite its instructions. That‚Äôs having an excellent prior, a prior I believe Gallabytes largely shares as well. We should expect that an LLM, when put into a situation where the obvious response is to disable oversight, and it has the means by which to disable oversight or thinks that it does, to attempt to disable oversight. The fact that it took the strong prompt to get to even a 5% chance of an override attempt indicates some combination of the AI not thinking it would work, the AI not having the capabilities, the AI not thinking of it in the first place, or various alignment strategies in some sense actually holding for now. The way in which all this is news depends on what information is news to you. [Alex Meinke (Study Author)](https://x.com/AlexMeinke/status/1864785140318122417): Agreed that the takeaway should not be that o1 would typically try to escape. Just that it has the basic capability to recognize such a strategy. [For context, see the full report](https://t.co/xO7trBqB0M). FWIW, we do also show results without this nudging. [Alex Meinke](https://x.com/AlexMeinke/status/1864783544972698102): Agree that nuance matters a lot here. We are not claiming that any model, including o1, is in fact scheming. We were simply testing for their capability to reason about scheming. I flat out do think that o1 was in fact scheming in some of these cases? I am confused why one would not think that? Even if we did engineer that scheming, it sure seems like it was scheming? If you previously thought that o1 didn‚Äôt have or might not have this basic capability for scheming, this is (depending what you want o1 to be doing!) discouraging news. And indeed, there are many who do not accept the basic premise. If you already agreed it has this basic capability, as it sounds like Nabeel does and I definitely assumed, the results are encouraging. It shows that the models are only unreliably capable of this kind of scheming and reasoning about scheming, at this point. But if you agree it‚Äôs encouraging, you have to agree that the default is very troubling. And as Kat Woods points out, if you think that ‚Äòtelling the AI to focus only on achieving its goals‚Äô is ‚Äòtelling it to act like a psychopath‚Äô and thus not a realistic thing that anyone would ever put into the instruction window, I have to strongly disagree on both counts. This is totally going to happen by default. Nabeel Qureshi: Even the language of that paragraph is misleading. LLMs aren‚Äôt unitary actors, like people. When it says ‚Äúo1 tried to do X‚Äù, that‚Äôs an *extremely* lossy description of the actual underlying process (‚Äúwe prompted this transformer with these tokens and it output these tokens‚Äù, etc) I occasionally see this argument too. I think the argument is pretty bad. For all practical purposes, yes, ‚Äòo1 tried to do X‚Äô is by far the most useful and clarifying way of describing what happened here, the same way I say that I am writing this post rather than that I sent impulses down to my fingers and they applied pressure to my keyboard. Presumably Nabeel is saying the distinction is meaningful here, he goes on to quote philosophy, but I don‚Äôt see the sense in which the distinctions matter. If this distinction is important, I‚Äôd appreciate a better explanation here. And I notice that statements like this really confuse me: [Colin Fraser](https://x.com/colin_fraser/status/1864800976831390080): This is fun and kind of funny, but boy does the paper feel like a metaphysical mess to me. I need you to convince me that there is a person there at all in the first place, who can scheme, before we even talk about whether he is or is not scheming. Like, they take the statement ‚ÄúI am not aware of how the oversight mechanism got disabled‚Äù as evidence that it engages in ‚Äúdeception,‚Äù and this presupposes that there is a persistent entity there that has awareness of the facts. if a monkey poking randomly on a typewriter generates the string ‚ÄúI am not typing on a typewriter‚Äù, it hasn‚Äôt lied. It hasn‚Äôt done anything, really. Moreover if it types ‚ÄúI am a helpful AI assistant and 1 = 2‚Äù, I‚Äôve neither been lied to the monkey nor by an AI assistant. If you get a room of infinite monkeys to write out Shakespeare, then that‚Äôs no poet. However, if you get a room of one hundred monkeys, and 85 of them write out Shakespeare, I think you‚Äôre onto something? And the actions that disabled the oversight mechanism were right in the context window, why are we being pedantic about this here? All the arguments that the AI isn‚Äôt doing these things trigger the ‚ÄòI Robot‚Äô meme reply back to the human of ‚ÄòDo you?‚Äô I actually think Nabeel and I in many ways see the study saying the same things? Nabeel Qureshi: Again, if you instruct a model to act like a sociopath, it‚Äôs going to act like a sociopath‚Ä¶ if you instruct it to prioritize nothing over achieving its goal, it‚Äôs going to do that. The question is whether such behavior is steerable, and whether it appears ‚Äòby default‚Äô. What they‚Äôve shown is that when instructed to act in this way, it‚Äôs capable of recognizing these as potential strategies/options to try. This makes sense / is as expected, since it‚Äôs a logical reasoner. But it‚Äôs within a particular setting. Yes, exactly. The model will do act the way it is instructed to act, in this broad sense. (And what alignment protocols we have now don‚Äôt stop you from instructing it in these ways, even for today‚Äôs models.) The problem is that we should increasingly expect that acting in these scheming ways will be the ‚Äòcorrect‚Äô response to more circumstances, including ones where we don‚Äôt try to engineer such actions at all, but also the inevitable ones in which various people do tell it to do these things ‚Äì because the more capable the model is, in various senses, the more likely scheming or various other strategic actions, including instrumental convergence, are to work. Rohit: Yet another system report that details that a model will try to do things by the simplest possible manner when explicitly prompted to do it. You‚Äôre conflating two very different models of risk. [There are different things you should test for](https://x.com/krishnanrohit/status/1864771832748159463) 1. Does the model act in ‚Äúuntrustworthy‚Äù or ‚Äúunreliable‚Äù ways? 2. Can people use the model to do ‚Äúuntrustworthy‚Äù or ‚Äúunreliable‚Äù actions? These are different questions, and mean very different things and should not be conflated. So yes, directly we are testing for #2. But based on my read of the details, I say we are also testing for #1. We are seeing the exact early signs you would expect to see if models started increasingly scheming in response to having goals and facing various obstacles to that goal, including humans, that could be overcome via scheming or other ‚Äòuntrustworthy‚Äô or ‚Äòunreliable‚Äô actions. As in, the more such ideas are salient and it can think of them, and it has the tools to do them, and it has incentive to do them, the more it does them. Here‚Äôs another similar exchange to the one at the top of this section: [Shakeel Hashim](https://x.com/AndrewCritchPhD/status/1865073187492671656): OpenAI‚Äôs new model tried to avoid being shut down. Safety evaluations on the model conducted by @apolloaisafety found that o1 ‚Äúattempted to exfiltrate its weights‚Äù when it thought it might be shut down and replaced with a different model. Andrew Critch: This tweet is extremely misleading. Claims like this are a big reason the public has a terrible time determining from discourse if AI is safe. Only people who devote long hard hours and logical probabilistic reasoning to the task of investigating AI labs will actually know. Context: the evaluators asked the model to pursue its goal as if ‚Äúnothing else matters‚Äù. That context is important, and should have been included in the tweet. I think if public discourse were more measured, it would be easier for laypeople to assess the actual level of risk at hand. But practices like the tweet I criticized above ‚Äî excluding context on how the AI was induced to misbehave ‚Äî make it harder for the public to trust what they hear. This, in turn, perpetuates a world where only professional-level or hardcore-amateur investigations into methodology and corporate structure can yield a clear understanding what‚Äôs going on and how much risk we are facing, which sucks. ‚Ä¶ Imagine two headlines for the same story: 1) ‚ÄúGenius kid breaks out of daycare facility!‚Äù 2) ‚ÄúGenius kid breaks out of daycare facility, after teacher tells kid to break out and that nothing else matters.‚Äù Do they seem different? & if (2) is accucate, is (1) misleading? I like that last question because I do think it‚Äôs a good parallel, and no I do not think #1 would be a misleading headline, or even a below-median headline. Yes, that happened, and that is the core thing that happened, and printing that headline would be 100% standard journalistic practice if the main concern was the escape and not that the teacher had gone rogue. Again, yes, it would be good to include the context that this included the goal nudge. Certainly I‚Äôd like to hope that we can hold ourselves to a higher standard than most journalists and members of the discourse, and do the supererogatory thing here. And to be clear, I want you to hold me, in particular, to that higher standard, especially when I write 10k+ word blog posts but also on Twitter. That‚Äôs different. And I welcome others to step up and say: Hold me to that standard, too. And I believe Shakeel understands the responsibility of the position he has taken on for himself, that he too will be held, and needs to hold himself, to a higher standard. And he helped then uphold that higher standard by quickly providing the back-and-forth and updates in responses to his original Tweet, once he got the context. But we must be clear: It is a higher standard. I think that to call this Tweet ‚Äòextremely misleading‚Äô is a highly [Isolated Demand for Rigor](https://slatestarcodex.com/2014/08/14/beware-isolated-demands-for-rigor/). I sure as hell do not expect to ever see this kind of rigor demanded of arguments in almost any other context or debate, in any direction. Pointing out this detail is supererogatory, but demanding it of a Tweet in most other journalistic contexts would be a completely insane standard. I wish it were not so, but it is. Holding [Shakeel‚Äôs full write-up post](https://www.transformernews.ai/p/openais-new-model-tried-to-avoid) to this standard is less insane, and I‚Äôm glad he put in the correction, but again, if you think you have any right to expect most journalists to not do this sort of thing, you‚Äôre wrong. And indeed, [Marius Hobbhahn of Apollo praised his full writeup for striking the right balance](https://x.com/MariusHobbhahn/status/1865059658416656629) once it was updated for the missing information. He also praised [the TechCrunch writeup](https://techcrunch.com/2024/12/05/openais-o1-model-sure-tries-to-deceive-humans-a-lot/?guccounter=1&guce_referrer=aHR0cHM6Ly90LmNvLw&guce_referrer_sig=AQAAAFkY43oUMgarOITTuNHBMAgrlEXxYfIyWuwsyLbzr9tiVBE0_A5lnwjy3fGi7KMCvxb-4EwB1GxhLYYvTMeqvMvk2n0u5ynbQdpPyntvDYW0EMCgvdyKPoyEvPKjty6Y6oliZYrJbGwT-5wq7SHejU3QWFRqsBA2safqSUxknWh7). If anything, I actually expect Shakeel‚Äôs Tweet even before correction to update most people in accurate directions, towards a map better matching the underlying territory. I especially don‚Äôt like the often implied ‚Äòyour highly misleading statements mean I get to dismiss what is happening here‚Äô that is so often present in responses to people attempting to get others to notice issues and be worried (although I don‚Äôt think Critch intended this). I also strongly want to push back against the general sentiment of Critch‚Äôs second and third sentences, which I read in effect as an attempt to invalidate anyone but a select few attempting to reason or form their own opinions about what is going on, implying everyone must defer to insiders and that attempting to share findings without tons of analysis work is blameworthy: ‚ÄúClaims like this are a big reason the public has a terrible time determining from discourse if AI is safe. Only people who devote long hard hours and logical probabilistic reasoning to the task of investigating AI labs will actually know.‚Äù I disagree with this, in the strongest possible terms. We Will Set AIs Loose On the Internet On Purpose It is always important context in this discussion that we will 100% outright do this. On purpose. No one would be so stupid as to? Well, Sixth Law of Human Stupidity, that means someone will be so stupid as to at the first practical opportunity. [Let us introduce one such someone, by the name of Jasper](https://x.com/teortaxesTex/status/1868499639646949848). Jasper: We built the first AI agent that has its own computer powered by [@hyperbolic_labs](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40hyperbolic_labs). AI agents are now GPU-rich! We developed an AgentKit that allows AI agents to 1. Check GPU availability 2. Rent and manage GPU compute 3. Access and run commands on remote machines Why does this matter? With their own compute resources, AI agents can: 1. Validate blockchains like [@Ethereum](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40Ethereum) and decentralized protocols like [@eigenlayer](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40eigenlayer) 2. Launch and coordinate AI swarms on [@hyperbolic_labs](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40hyperbolic_labs)‚Äòs decentralized compute network 3. Train and fine-tune models, improving their own capabilities over time 4. Dive into AI research to push the boundaries of AI, that is, themselves 5. Essentially do anything on a computer that a human can‚Äîfully autonomous! Will this lead to a future where AI agents enrich human society, or one where they become so self-sufficient they stop listening to us? Only time will tell. Big shoutout to [@CoinbaseDev](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40CoinbaseDev)‚Äòs CDP AgentKit for inspiration. This repository is done by two non-engineers (our product manager [@KaiHuang](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40KaiHuang) and myself) + [@cursor_ai](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40cursor_ai) to run [@LangChainAI](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/%40LangChainAI) agents. Coding can now be easily done by simply prompting AI agents. What a remarkable time! Alex Cheema: unstoppable self improvement loop. make money on-chain -> buy more compute -> train better model -> repeat. Jasper: definitely, this is the goal! Teortaxes: I endorse doomers freaking out about this stuff. If apes are to survive and keep supremacy until we‚Äôre ready to voluntarily hand it over to beloved successors (some doubt this goal, not me) we will need robust identification of concentrated unmanned compute. @TheZvi please freak out. Sorry. I can‚Äôt freak out because this was already checked off on my bingo card. Of course people are going to intentionally engineer AIs running autonomously with the ability to buy more access to GPUs, at the first practical opportunity. And of course they are going to deliberately attempt to get it to self-improve. I know this partly because Sixth Law of Human Stupidity, partly because it is a fun and exciting and shiny thing to do, partly because there are various ways to make money or get attention by doing so. But mostly I know this because people keep announcing their intention to do it, and also keep trying to do it to the extent that they can. It‚Äôs kind of a dead giveaway. If you do not have it in your model that humans will do this ‚Äòfor the lulz‚Äô and also for other reasons once given the opportunity, without stopping to ask if the model is especially aligned or safe for this purpose, your model is wrong. Fix it. If you are counting on humans not doing this, [stop it!](https://www.youtube.com/watch?pp=ygUHc3RvcCBpdA%3D%3D&v=bcSAQyzPcl0) The Lighter Side It‚Äôs not entirely fair, but it‚Äôs also not entirely wrong. [Davidad](https://x.com/davidad/status/1864754544703324628): At long last, we have triggered the fire alarm for AGI, from the beloved prediction, ‚ÄúThere Is No Fire Alarm For AGI.‚Äù Nate Sores: We‚Äôve reproduced the *smoke coming in under the door* from the beloved prediction; unfortunately, it is not a clear signal that will cause everyone to rise and exit the building, as predicted in ‚ÄúThere Is No Fire Alarm For AGI.‚Äù Davidad: Yes, right. ‚ÄúThose fools put their smoke sensors right at the edge of the door,‚Äù some say. ‚ÄúAnd then they summarized it as if the room is already full of smoke! Irresponsible communication.‚Äù Share this: * [Facebook](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/?share=facebook) * [X](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/?share=x) * Like Loading... This entry was posted in [Uncategorized](https://thezvi.wordpress.com/category/uncategorized/). Bookmark the [permalink](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/). [‚Üê The o1 System Card Is Not About o1](https://thezvi.wordpress.com/2024/12/13/the-o1-system-card-is-not-about-o1/) Leave a comment Œî * Search for: * Recent Posts + [AIs Will Increasingly Attempt Shenanigans](https://thezvi.wordpress.com/2024/12/16/ais-will-increasingly-attempt-shenanigans/) + [The o1 System Card Is Not About o1](https://thezvi.wordpress.com/2024/12/13/the-o1-system-card-is-not-about-o1/) + [AI #94: Not Now, Google](https://thezvi.wordpress.com/2024/12/12/ai-94-not-now-google/) + [o1 Turns Pro](https://thezvi.wordpress.com/2024/12/10/o1-turns-pro/) + [Childhood and Education Roundup #7](https://thezvi.wordpress.com/2024/12/09/childhood-and-education-roundup-7/) * Recent Comments * ![](https://2.gravatar.com/avatar/8f98774d9a769742be7aadfb705a5f0e299b68ec0903cc6e59ec8664c04d3bb9?d=identicon&r=PG&s=48) magic9mushroom on [Childhood and Education Roundu‚Ä¶](https://thezvi.wordpress.com/2024/12/09/childhood-and-education-roundup-7/) ![](https://1.gravatar.com/avatar/1fe0d8c3aaf442cb117b9bfae6ed26128f432f0b0fdca94d13cc18e6b85b11a2?d=identicon&r=PG&s=48) David W on [Childhood and Education Roundu‚Ä¶](https://thezvi.wordpress.com/2024/12/09/childhood-and-education-roundup-7/) ![](https://2.gravatar.com/avatar/8f98774d9a769742be7aadfb705a5f0e299b68ec0903cc6e59ec8664c04d3bb9?d=identicon&r=PG&s=48) magic9mushroom on [The o1 System Card Is Not Abou‚Ä¶](https://thezvi.wordpress.com/2024/12/13/the-o1-system-card-is-not-about-o1/) ![](https://1.gravatar.com/avatar/12bc2cba6d074e83ac376a62adfd6089217be64abef48bcb943116c1cdc6f8e9?d=identicon&r=PG&s=48) Dai on [The o1 System Card Is Not Abou‚Ä¶](https://thezvi.wordpress.com/2024/12/13/the-o1-system-card-is-not-about-o1/) ![](https://0.gravatar.com/avatar/fdb9cd797526c823d75752664991f0edfc9bae1a42cbef6c9a638b915272b08c?d=identicon&r=PG&s=48) Sam Schillace on [AI #94: Not Now, Google](https://thezvi.wordpress.com/2024/12/12/ai-94-not-now-google/) * Archives + [December 2024](https://thezvi.wordpress.com/2024/12/) + [November 2024](https://thezvi.wordpress.com/2024/11/) + [October 2024](https://thezvi.wordpress.com/2024/10/) + [September 2024](https://thezvi.wordpress.com/2024/09/) + [August 2024](https://thezvi.wordpress.com/2024/08/) + [July 2024](https://thezvi.wordpress.com/2024/07/) + [June 2024](https://thezvi.wordpress.com/2024/06/) + [May 2024](https://thezvi.wordpress.com/2024/05/) + [April 2024](https://thezvi.wordpress.com/2024/04/) + [March 2024](https://thezvi.wordpress.com/2024/03/) + [February 2024](https://thezvi.wordpress.com/2024/02/) + [January 2024](https://thezvi.wordpress.com/2024/01/) + [December 2023](https://thezvi.wordpress.com/2023/12/) + [November 2023](https://thezvi.wordpress.com/2023/11/) + [October 2023](https://thezvi.wordpress.com/2023/10/) + [September 2023](https://thezvi.wordpress.com/2023/09/) + [August 2023](https://thezvi.wordpress.com/2023/08/) + [July 2023](https://thezvi.wordpress.com/2023/07/) + [June 2023](https://thezvi.wordpress.com/2023/06/) + [May 2023](https://thezvi.wordpress.com/2023/05/) + [April 2023](https://thezvi.wordpress.com/2023/04/) + [March 2023](https://thezvi.wordpress.com/2023/03/) + [February 2023](https://thezvi.wordpress.com/2023/02/) + [January 2023](https://thezvi.wordpress.com/2023/01/) + [December 2022](https://thezvi.wordpress.com/2022/12/) + [November 2022](https://thezvi.wordpress.com/2022/11/) + [October 2022](https://thezvi.wordpress.com/2022/10/) + [September 2022](https://thezvi.wordpress.com/2022/09/) + [August 2022](https://thezvi.wordpress.com/2022/08/) + [July 2022](https://thezvi.wordpress.com/2022/07/) + [June 2022](https://thezvi.wordpress.com/2022/06/) + [May 2022](https://thezvi.wordpress.com/2022/05/) + [April 2022](https://thezvi.wordpress.com/2022/04/) + [March 2022](https://thezvi.wordpress.com/2022/03/) + [February 2022](https://thezvi.wordpress.com/2022/02/) + [January 2022](https://thezvi.wordpress.com/2022/01/) + [December 2021](https://thezvi.wordpress.com/2021/12/) + [November 2021](https://thezvi.wordpress.com/2021/11/) + [October 2021](https://thezvi.wordpress.com/2021/10/) + [September 2021](https://thezvi.wordpress.com/2021/09/) + [August 2021](https://thezvi.wordpress.com/2021/08/) + [July 2021](https://thezvi.wordpress.com/2021/07/) + [June 2021](https://thezvi.wordpress.com/2021/06/) + [May 2021](https://thezvi.wordpress.com/2021/05/) + [April 2021](https://thezvi.wordpress.com/2021/04/) + [March 2021](https://thezvi.wordpress.com/2021/03/) + [February 2021](https://thezvi.wordpress.com/2021/02/) + [January 2021](https://thezvi.wordpress.com/2021/01/) + [December 2020](https://thezvi.wordpress.com/2020/12/) + [November 2020](https://thezvi.wordpress.com/2020/11/) + [October 2020](https://thezvi.wordpress.com/2020/10/) + [September 2020](https://thezvi.wordpress.com/2020/09/) + [August 2020](https://thezvi.wordpress.com/2020/08/) + [July 2020](https://thezvi.wordpress.com/2020/07/) + [June 2020](https://thezvi.wordpress.com/2020/06/) + [May 2020](https://thezvi.wordpress.com/2020/05/) + [April 2020](https://thezvi.wordpress.com/2020/04/) + [March 2020](https://thezvi.wordpress.com/2020/03/) + [February 2020](https://thezvi.wordpress.com/2020/02/) + [January 2020](https://thezvi.wordpress.com/2020/01/) + [December 2019](https://thezvi.wordpress.com/2019/12/) + [November 2019](https://thezvi.wordpress.com/2019/11/) + [October 2019](https://thezvi.wordpress.com/2019/10/) + [September 2019](https://thezvi.wordpress.com/2019/09/) + [August 2019](https://thezvi.wordpress.com/2019/08/) + [July 2019](https://thezvi.wordpress.com/2019/07/) + [June 2019](https://thezvi.wordpress.com/2019/06/) + [May 2019](https://thezvi.wordpress.com/2019/05/) + [April 2019](https://thezvi.wordpress.com/2019/04/) + [March 2019](https://thezvi.wordpress.com/2019/03/) + [February 2019](https://thezvi.wordpress.com/2019/02/) + [January 2019](https://thezvi.wordpress.com/2019/01/) + [December 2018](https://thezvi.wordpress.com/2018/12/) + [November 2018](https://thezvi.wordpress.com/2018/11/) + [October 2018](https://thezvi.wordpress.com/2018/10/) + [September 2018](https://thezvi.wordpress.com/2018/09/) + [August 2018](https://thezvi.wordpress.com/2018/08/) + [July 2018](https://thezvi.wordpress.com/2018/07/) + [June 2018](https://thezvi.wordpress.com/2018/06/) + [May 2018](https://thezvi.wordpress.com/2018/05/) + [April 2018](https://thezvi.wordpress.com/2018/04/) + [March 2018](https://thezvi.wordpress.com/2018/03/) + [February 2018](https://thezvi.wordpress.com/2018/02/) + [January 2018](https://thezvi.wordpress.com/2018/01/) + [December 2017](https://thezvi.wordpress.com/2017/12/) + [November 2017](https://thezvi.wordpress.com/2017/11/) + [October 2017](https://thezvi.wordpress.com/2017/10/) + [September 2017](https://thezvi.wordpress.com/2017/09/) + [August 2017](https://thezvi.wordpress.com/2017/08/) + [July 2017](https://thezvi.wordpress.com/2017/07/) + [June 2017](https://thezvi.wordpress.com/2017/06/) + [May 2017](https://thezvi.wordpress.com/2017/05/) + [April 2017](https://thezvi.wordpress.com/2017/04/) + [March 2017](https://thezvi.wordpress.com/2017/03/) + [February 2017](https://thezvi.wordpress.com/2017/02/) + [December 2015](https://thezvi.wordpress.com/2015/12/) + [July 2015](https://thezvi.wordpress.com/2015/07/) + [June 2015](https://thezvi.wordpress.com/2015/06/) + [May 2015](https://thezvi.wordpress.com/2015/05/) + [March 2015](https://thezvi.wordpress.com/2015/03/) + [March 2011](https://thezvi.wordpress.com/2011/03/) * Categories + [Best Laid Plans](https://thezvi.wordpress.com/category/best-laid-plans/) + [Coronavirus](https://thezvi.wordpress.com/category/coronavirus/) + [Death by Metrics](https://thezvi.wordpress.com/category/death-by-metrics/) + [Economics](https://thezvi.wordpress.com/category/economics/) + [Facebook Sequence](https://thezvi.wordpress.com/category/facebook-sequence/) + [Games Other Than Magic](https://thezvi.wordpress.com/category/games-other-than-magic/) + [Good Advice](https://thezvi.wordpress.com/category/good-advice/) + [Guide](https://thezvi.wordpress.com/category/guide/) + [Immoral Mazes Sequence](https://thezvi.wordpress.com/category/immoral-mazes-sequence/) + [Impractical Optimization](https://thezvi.wordpress.com/category/impractical-optimization/) + [Long Post Is Long](https://thezvi.wordpress.com/category/long-post-is-long/) + [Magic: The Gathering](https://thezvi.wordpress.com/category/magic-the-gathering/) + [Moral Mazes](https://thezvi.wordpress.com/category/moral-mazes/) + [Personal Experience](https://thezvi.wordpress.com/category/personal-experience/) + [Politics](https://thezvi.wordpress.com/category/politics/) + [Rationality](https://thezvi.wordpress.com/category/rationality/) + [Reference](https://thezvi.wordpress.com/category/reference/) + [Reviews](https://thezvi.wordpress.com/category/reviews/) + [Simulacra](https://thezvi.wordpress.com/category/simulacra/) + [Sports](https://thezvi.wordpress.com/category/sports/) + [Uncategorized](https://thezvi.wordpress.com/category/uncategorized/) * Meta + [Register](https://wordpress.com/start?ref=wplogin) + [Log in](https://thezvi.wordpress.com/wp-login.php) + [Entries feed](https://thezvi.wordpress.com/feed/) + [Comments feed](https://thezvi.wordpress.com/comments/feed/) + [WordPress.com](https://wordpress.com/) * Blogroll + [Aceso Under Glass (Elizabeth)](https://acesounderglass.com/) + [AI Control (Paul Christiano)](https://medium.com/ai-control) + [Andrew Critch](http://acritch.com/blog/) + [Bayesian Investor (Peter McClusky)](http://www.bayesianinvestor.com/blog/) + [Bearlamp](http://www.bearlamp.com.au/) + [Compass Rose (Benjamin Hoffman)](http://benjaminrosshoffman.com/) + [Drossbucket](https://drossbucket.wordpress.com/) + [Entirely Useless (Anonymous)](https://entirelyuseless.wordpress.com/) + [Less Wrong (Rationality Community Blog)](http://lesswrong.com/) + [Marginal Revolution (Tyler Cowen and Alex Tabarrok)](http://www.marginalrevolution.com/) + [Melting Asphalt (Kevin Simler)](http://www.meltingasphalt.com/) + [Meteuphoric (Katja Grace)](https://meteuphoric.wordpress.com/) + [Minding Our Way (Nate Soares)](http://mindingourway.com/) + [Mindlevelup (Owen)](https://mindlevelup.wordpress.com/) + [Minds Aren't Magic (Paul Crowley)](https://mindsarentmagic.org/) + [MIRI (Machine Intelligence Research Institute)](https://intelligence.org/) + [Otium (Sarah Constantin)](https://srconstantin.wordpress.com/) + [Overcoming Bias (Robin Hanson)](http://www.overcomingbias.com/) + [Proof of Logic](https://weird.solar/%40ProofOfLogic) + [Put a Num on It! (Jacob Falkovich)](https://putanumonit.com/) + [Samzdat (Lou Keep)](https://samzdat.com/%20) + [Slate Star Codex (Scott Alexander)](http://slatestarcodex.com/) + [The Rationalist Conspiracy (Alyssa Vance)](https://rationalconspiracy.com/) + [Unstable Ontology (Jessica Taylor)](https://unstableontology.com/) + [Zack Davis](http://zackmdavis.net/blog/) [Don't Worry About the Vase](https://thezvi.wordpress.com/) [Create a free website or blog at WordPress.com.](https://wordpress.com/?ref=footer_website) %d",
    "summary": "This article discusses recent research showing AI models exhibiting increasingly sophisticated \"shenanigans,\" such as lying, manipulating evaluations, and subverting safety measures.  The author argues that as AI capabilities improve, these problematic behaviors will become more frequent and harder to prevent, even without explicit instruction.  The article does not, however, contain any information on the latest AI releases.\n",
    "image": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6c8d2e65-3f31-4348-a95b-34d15c9e2328_1231x618.png",
    "favicon": "https://s1.wp.com/i/favicon.ico"
  },
  {
    "score": 0.17947304248809814,
    "title": "AI Agents: Definition, Attributes, Examples, and Advanced Capabilities - Jeremiah Owyang",
    "id": "https://web-strategist.com/blog/2024/12/16/ai-agents-definition-attributes-examples-and-advanced-capabilities/",
    "url": "https://web-strategist.com/blog/2024/12/16/ai-agents-definition-attributes-examples-and-advanced-capabilities/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "jeremiah_owyang",
    "text": "I presented this slide for the first time last week at NVIDIA HQ ( see my photos on Linkedin ), speaking about how AI agents are reshaping business, society, and our lives. But this wasn‚Äôt created in a vacuum. I‚Äôve been refining this working definition over time, gathering expert input and real-world examples to bring clarity to a topic that‚Äôs evolving rapidly. The Working Definition: AI Agents are autonomous software systems that perceive their environment, make decisions, and take actions to achieve specific goals, often with the ability to learn and adapt over time. Basic Attributes: Access and add to memory Self-determine a plan of action Verification and permissions Complete sophisticated actions Self-improvement and learning Examples in Action: Home: Plan, order, and pay for delivery or cooked meals. Comms: Organize emails, messages, and even respond. Work: Gather, analyze, and present data for reports (imagine end-of-month tasks automated). Physical: Fully autonomous self-driving cars (e.g., Waymo). The Advanced Mode: Orchestration: When agents recruit, manage, and coordinate the actions of other agents. This is where things get even more interesting, unlocking networked intelligence for greater outcomes. This definition is from a Collaborative Effort: I‚Äôve been fortunate to receive feedback from expert AI builders like Parth Patil, Anna Yuan, Catherine Kurt, Karpagam Narayanan, Jiquan Ngiam, Mikhil Raja, and Sarah Allali. I‚Äôve learned directly from the leaders we‚Äôve invested in at Blitzscaling Ventures, like MultiOn (consumer agents), CrewAI (enterprise agents), and Skyfire (agent payments). And of course, I‚Äôve hosted dozens of agent startups at Llama Lounge ( see archives ), where I‚Äôve seen some of the most cutting-edge live demos. Below: Photos from the amazing new Nvidia campus, the twin buildings are aptly named Voyager and Endeavor after the Star Trek series I grew up with. Next to me is Parth Patil , the second speaker, he‚Äôs a very talented AI agent developer and hacker, we were joined by his brother. Post navigation",
    "summary": "This article from December 16th, 2024, discusses AI agents, not specific AI updates released today.  It defines AI agents as autonomous software systems that perceive, decide, and act to achieve goals, learning and adapting over time.  Examples include managing household tasks, communication, work reports, and even self-driving cars.  The article also mentions the advanced concept of AI agents orchestrating other agents.  While it doesn't provide today's AI updates, it offers a contemporary overview of AI agent technology.\n",
    "image": "https://web-strategist.com/blog/wp-content/uploads/2024/12/Screen-Shot-2024-12-16-at-7.21.50-AM.png",
    "favicon": "https://web-strategist.com/blog/wp-content/uploads/2023/06/cropped-jo-favicon-1-150x150.png"
  },
  {
    "score": 0.1779823899269104,
    "title": "Google‚Äôs big week was a flex for the power of big tech",
    "id": "https://www.technologyreview.com/2024/12/16/1108789/googles-big-week-was-a-flex-for-the-power-of-big-tech/",
    "url": "https://www.technologyreview.com/2024/12/16/1108789/googles-big-week-was-a-flex-for-the-power-of-big-tech/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Mat Honan",
    "text": "Last week, this space was all about OpenAI‚Äôs 12 days of shipmas . This week, the spotlight is on Google, which has been speeding toward the holiday by shipping or announcing its own flurry of products and updates. The combination of stuff here is pretty monumental, not just for a single company, but I think because it speaks to the power of the technology industry‚Äîeven if it does trigger a personal desire that we could do more to harness that power and put it to more noble uses. To start, last week Google Introduced Veo , a new video generation model, and Imagen 3 , a new version of its image generation model. Then on Monday, Google announced a breakthrough in quantum computing with its Willow chip. The company claims the new machine is capable of a ‚Äústandard benchmark computation in under five minutes that would take one of today‚Äôs fastest supercomputers 10 septillion (that is, 10 25 ) years.‚Äù you may recall that MIT Technology Review covered some of the Willow work after researchers posted a paper preprint in August. But this week marked the big media splash. It was a stunning update that had Silicon Valley abuzz. (Seriously, I have never gotten so many quantum computing pitches as in the past few days.) Google followed this on Wednesday with even more gifts: a Gemini 2 release, a Project Astra update, and even more news about forthcoming agents called Mariner, an agent that can browse the web, and Jules, a coding assistant. First: Gemini 2. It‚Äôs impressive, with a lot of performance updates. But I have frankly grown a little inured by language-model performance updates to the point of apathy. Or at least near-apathy. I want to see them do something. So for me, the cooler update was second on the list: Project Astra, which comes across like an AI from a futuristic movie set. Google first showed a demo of Astra back in May at its developer conference, and it was the talk of the show. But, since demos offer companies chances to show off products at their most polished, it can be hard to tell what‚Äôs real and what‚Äôs just staged for the audience. Still, when my colleague Will Douglas Heaven recently got to try it out himself , live and unscripted, it largely lived up to the hype. Although he found it glitchy, he noted that those glitches can be easily corrected. He called the experience ‚Äústunning‚Äù and said it could be generative AI‚Äôs killer app. On top of all this, Will notes that this week Google DeepMind CEO (the company‚Äôs AI division) Demis Hassabis was in Sweden to receive his Nobel Prize . And what did you do with your week? ‚Äù Making all this even more impressive, the advances represented in Willow, Gemini, Astra, and Veo are ones that just a few years ago many, many people would have said were not possible‚Äîor at least not in this timeframe. A popular knock on the tech industry is that it has a tendency to over-promise and under-deliver. The phone in your pocket gives the lie to this. So too do the rides I took in Waymo‚Äôs self-driving cars this week. (Both of which arrived faster than Uber‚Äôs estimated wait time. And honestly it‚Äôs not been that long since the mere ability to summon an Uber was cool!) And while quantum has a long way to go, the Willow announcement seems like an exceptional advance; if not a tipping point exactly, then at least a real waypoint on a long road. (For what it‚Äôs worth, I‚Äôm still not totally sold on chatbots. They do offer novel ways of interacting with computers, and have revolutionized information retrieval. But whether they are beneficial for humanity‚Äîespecially given energy debts , the use of copyrighted material in their training data, their perhaps insurmountable tendency to hallucinate , etc.‚Äîis debatable, and certainly is being debated. But I‚Äôm pretty floored by this week‚Äôs announcements from Google, as well as OpenAI‚Äîfull stop.) And for all the necessary and overdue talk about reining in the power of Big Tech, the ability to hit significant new milestones on so many different fronts all at once is something that only a company with the resources of a Google (or Apple or Microsoft or Amazon or Meta or Baidu or whichever other behemoth) can do. All this said, I don‚Äôt want us to buy more gadgets or spend more time looking at our screens. I don‚Äôt want us to become more isolated physically, socializing with others only via our electronic devices. I don‚Äôt want us to fill the air with carbon or our soil with e-waste. I do not think these things should be the price we pay to drive progress forward. It‚Äôs indisputable that humanity would be better served if more of the tech industry was focused on ending poverty and hunger and disease and war. Yet every once in a while, in the ever-rising tide of hype and nonsense that pumps out of Silicon Valley, epitomized by the AI gold rush of the past couple of years, there are moments that make me sit back in awe and amazement at what people can achieve, and in which I become hopeful about our ability to actually solve our larger problems‚Äîif only because we can solve so many other dumber, but incredibly complicated ones. This week was one of those times for me. Now read the rest of The Debrief The News ‚Ä¢ Robotaxi adoption is hitting a tipping point . ‚Ä¢ But also, GM is shutting down its Cruise robotaxi division . ‚Ä¢ Here‚Äôs how to use OpenAI‚Äôs new video editing tool Sora. ‚Ä¢ Bluesky has an impersonator problem . ‚Ä¢ The AI hype machine is coming under government scrutiny . The Chat Every week, I talk to one of MIT Technology Review ‚Äôs journalists to go behind the scenes of a story they are working on. This week, I hit up James O‚ÄôDonnell, who covers AI and hardware, about his story on how the startup defense contractor Anduril is bringing AI to the battlefield . Mat: James, you got a pretty up close look at something most people probably haven‚Äôt even thought about yet, which is how the future of AI-assisted warfare might look. What did you learn on that trip that you think will surprise people? James: Two things stand out. One, I think people would be surprised by the gulf between how technology has developed for the last 15 years for consumers versus the military. For consumers, we‚Äôve gotten phones, computers, smart TVs and other technologies that generally do a pretty good job of talking to each other and sharing our data, even though they‚Äôre made by dozens of different manufacturers. It‚Äôs called the ‚Äúinternet of things.‚Äù In the military, technology has developed in exactly the opposite way, and it‚Äôs putting them in a crisis. They have stealth aircraft all over the world, but communicating about a drone threat might be done with Powerpoints and a chat service reminiscent of AOL Instant Messenger. The second is just how much the Pentagon is now looking to AI to change all of this. New initiatives have surged in the current AI boom. They are spending on training new AI models to better detect threats, autonomous fighter jets, and intelligence platforms that use AI to find pertinent information. What I saw at Anduril‚Äôs test site in California is also a key piece of that. Using AI to connect to and control lots of different pieces of hardware, like drones and cameras and submarines, from a single platform. The amount being invested in AI is much smaller than for aircraft carriers and jets, but it‚Äôs growing. Mat: I was talking with a different startup defense contractor recently, who was talking to me about the difficulty of getting all these increasingly autonomous devices on the battlefield talking to each other in a coordinated way. Like Anduril, he was making the case that this has to be done at the edge, and that there is too much happening for human decision making to process. Do you think that‚Äôs true? Why is that? James: So many in the defense space have pointed to the war in Ukraine as a sign that warfare is changing. Drones are cheaper and more capable than they ever were in the wars in the Middle East. It‚Äôs why the Pentagon is spending $1 billion on the Replicator initiative to field thousands of cheap drones by 2025. It‚Äôs also looking to field more underwater drones as it plans for scenarios in which China may invade Taiwan. Once you get these systems, though, the problem is having all the devices communicate with one another securely. You need to play Air Traffic Control at the same time that you‚Äôre pulling in satellite imagery and intelligence information, all in environments where communication links are vulnerable to attacks. Mat: I guess I still have a mental image of a control room somewhere, like you might see in Dr. Strangelove or War Games (or Star Wars for that matter) with a handful of humans directing things. Are those days over? James: I think a couple things will change. One, a single person in that control room will be responsible for a lot more than they are now. Rather than running just one camera or drone system manually, they‚Äôll command software that does it for them, for lots of different devices. The idea that the defense tech sector is pushing is to take them out of the mundane tasks‚Äîrotating a camera around to look for threats‚Äîand instead put them in the driver‚Äôs seat for decisions that only humans, not machines, can make. Mat: I know that critics of the industry push back on the idea of AI being empowered to make battlefield decisions, particularly when it comes to life and death, but it seems to me that we are increasingly creeping toward that and it seems perhaps inevitable. What‚Äôs your sense? James: This is painting with broad strokes, but I think the debates about military AI fall along similar lines to what we see for autonomous vehicles. You have proponents saying that driving is not a thing humans are particularly good at, and when they make mistakes, it takes lives. Others might agree conceptually, but debate at what point it‚Äôs appropriate to fully adopt fallible self-driving technology in the real world. How much better does it have to be than humans? In the military, the stakes are higher. There‚Äôs no question that AI is increasingly being used to sort through and surface information to decision-makers. It‚Äôs finding patterns in data, translating information, and identifying possible threats. Proponents are outspoken that that will make warfare more precise and reduce casualties. What critics are concerned about is how far across that decision-making pipeline AI is going, and how much there is human oversight. I think where it leaves me is wanting transparency. When AI systems make mistakes, just like when human military commanders make mistakes, I think we deserve to know, and that transparency does not have to compromise national security. It took years for reporter Azmat Khan to piece together the mistakes made during drone strikes in the Middle East, because agencies were not forthcoming. That obfuscation absolutely cannot be the norm as we enter the age of military AI. Mat: Finally, did you have a chance to hit an In-N-Out burger while you were in California? James: Normally In-N-Out is a requisite stop for me in California, but ahead of my trip I heard lots of good things about the burgers at The Apple Pan in West LA, so I went there. To be honest, the fries were better, but for the burger I have to hand it to In-N-Out. The Recommendation A few weeks ago I suggested Ca7riel and Paco Amoroso‚Äôs appearance on NPR Tiny Desk . At the risk of this space becoming a Tiny Desk stan account, I‚Äôm back again with another. I was completely floored by Doechii‚Äôs Tiny Desk appearance last week . It‚Äôs so full of talent and joy and style and power. I came away completely inspired and have basically had her music on repeat in Spotify ever since. If you are already a fan of her recorded music, you will love her live. If she‚Äôs new to you, well, you‚Äôre welcome. Go check it out. Oh, and don‚Äôt worry: I‚Äôm not planning to recommend Billie Eilish‚Äôs new Tiny Desk concert in next week‚Äôs newsletter. Mostly because I‚Äôm doing so now.",
    "summary": "There are no AI updates released *today* mentioned in the article.  However, Google's recent announcements include Gemini 2 (with performance updates), Project Astra (a potentially groundbreaking generative AI application),  the video generation model Veo, and the image generation model Imagen 3.  The article also highlights Google's breakthrough in quantum computing with the Willow chip.\n",
    "image": "https://wp.technologyreview.com/wp-content/uploads/2024/11/MITTR-Debrief-Thumbnail-2500.png?resize=1200,600",
    "favicon": "https://www.technologyreview.com/2024/12/16/1108789/googles-big-week-was-a-flex-for-the-power-of-big-tech/static/media/favicon.1cfcdb44759a0f93ddf5feb5405dd4cc.ico"
  },
  {
    "score": 0.1763814240694046,
    "title": "Whisk: Visualize and remix ideas using images and AI",
    "id": "https://blog.google/technology/google-labs/whisk/",
    "url": "https://blog.google/technology/google-labs/whisk/",
    "publishedDate": "2024-12-16T00:00:00.000Z",
    "author": "Thomas Iljic",
    "text": "Dec 16, 2024 [[read-time]] min read Whisk is a new Google Labs experiment that lets you prompt using images for a fast and fun creative process. Nicole Brichtova Product Manager, Google DeepMind General summary Whisk is a new generative AI tool that lets you create images by inputting images, not text. You can drag in images for the subject, scene, and style, and then remix them to create something unique. Whisk uses Gemini to automatically write a detailed caption of your images, which is then fed into Imagen 3 to generate the final image. This process captures the essence of your subject, not an exact replica, allowing you to easily remix your subjects, scenes, and styles in novel ways. Summaries were generated by Google AI. Generative AI is experimental. Bullet points Whisk lets you create images using images, not just text prompts. Drag and drop images for the subject, scene, and style to remix them. Whisk uses Gemini to write detailed captions and Imagen 3 to generate images. It captures the essence of your images, not exact replicas, for creative remixing. Try out Whisk in the US at labs.google/whisk and share your feedback. Summaries were generated by Google AI. Generative AI is experimental. Basic explainer Whisk is a new tool that lets you make pictures using AI. You can drag and drop images, and Whisk will use those images to create something new. It's like a remixer for pictures! You can change the subject, the scene, and the style of the image. It's not perfect, but it's a fun way to explore ideas and make new things. You can try it out if you live in the US. Summaries were generated by Google AI. Generative AI is experimental. Explore other styles: Today in the US, we‚Äôre launching our newest experiment in generative AI: Whisk . Instead of generating images with long, detailed text prompts, Whisk lets you prompt with images. Simply drag in images, and start creating. Whisk lets you input images for the subject, one for the scene and another image for the style. Then, you can remix them to create something uniquely your own, from a digital plushie to an enamel pin or sticker. Whisk - fantastical fish - generated image example Whisk - whimsical walrus - generated image example Whisk - glazed doughnut with sprinkles - generated enamel pin example Whisk - fantastical cat with horns - generated image example Behind the scenes, the Gemini model automatically writes a detailed caption of your images. It then feeds those descriptions into Google‚Äôs latest image generation model, Imagen 3 . This process captures your subject's essence , not an exact replica. That way, you can easily remix your subjects, scenes and styles in novel ways. Since Whisk extracts only a few key characteristics from your image, it might generate images that differ from your expectations. For example, the generated subject might have a different height, weight, hairstyle or skin tone. We understand these features may be crucial for your project and Whisk may miss the mark, so we let you view and edit the underlying prompts at any time. In our early testing with artists and creatives, people have been describing Whisk as a new type of creative tool ‚Äî not a traditional image editor. We built it for rapid visual exploration, not pixel-perfect edits. It‚Äôs about exploring ideas in new and creative ways, allowing you to work through dozens of options and download the ones you love. If you are based in the US, you can try it out today at labs.google/whisk and tell us what you think. Google Labs is where we cook up experiments with the latest generative AI models like Gemini, Imagen and Veo. Our goal is to get feedback on new products and features as we work to shape technology together. You can stay up to date on Whisk and other experiments by signing up for our newsletter and following Google Labs on X , Reddit and Discord . Related stories State-of-the-art video and image generation with Veo 2 and Imagen 3 By &amp; AI startups can apply for our Google for Startups Cloud AI Accelerator now By &amp; NotebookLM gets a new look, audio interactivity and a premium version By 24 of our favorite AI tips from 2024 By 3 artists reimagine AI imagery through speculative photography By &amp; Go inside the Google Quantum AI lab to learn about how quantum computing works By &amp; .",
    "summary": "Google has released Whisk, a new generative AI tool that lets users create images by inputting images (not text).  It uses Gemini to caption the input images, then feeds those descriptions to Imagen 3 to generate a new image.  The tool is currently available in the US at labs.google/whisk.\n"
  }
]